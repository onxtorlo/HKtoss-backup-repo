### A100 80GB 긴급 수정 설정 (학습률 정상화)
model_name: "allganize/Llama-3-Alpha-Ko-8B-Instruct"
dataset_path: "."
max_seq_length: 1024
output_dir: "./llama-3.1-korean-8b-hf-stable"
report_to: "wandb"
learning_rate: 0.00003                    # 0.000005 → 0.00003 (6배 증가)
lr_scheduler_type: "constant"             # cosine → constant (고정 학습률)
num_train_epochs: 30                      # 35 → 30 (적당히)
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 12           # 16 → 12 (조금 줄임)
optim: "adamw_bnb_8bit"
logging_steps: 1
save_strategy: "steps"
save_steps: 15
eval_strategy: "steps"
eval_steps: 15
weight_decay: 0.01                        # 0.015 → 0.01 (덜 제한적)
max_grad_norm: 0.5                        # 0.3 → 0.5 (덜 제한적)
warmup_ratio: 0.05                        # 0.25 → 0.05 (최소 워밍업)
bf16: true
tf32: true
gradient_checkpointing: true
dataloader_num_workers: 2
group_by_length: false
load_best_model_at_end: false
metric_for_best_model: "eval_loss"
greater_is_better: false
save_total_limit: 3
evaluation_strategy: "steps"
early_stopping_patience: 5
dataloader_pin_memory: false
remove_unused_columns: true

# 안정화 설정 (간소화)
dataloader_drop_last: true
seed: 42
data_seed: 42
fp16_full_eval: false