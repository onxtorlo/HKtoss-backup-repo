
import time
from huggingface_hub import HfApi
from dotenv import load_dotenv
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

torch.cuda.empty_cache()
torch.cuda.synchronize()

# ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
allocated = torch.cuda.memory_allocated() / 1024**3
reserved = torch.cuda.memory_reserved() / 1024**3
print(f"GPU ë©”ëª¨ë¦¬: {allocated:.1f}GB ì‚¬ìš© ì¤‘, {reserved:.1f}GB ì˜ˆì•½ë¨")

def section_by_section(input_text: str, tokenizer, model) -> str:
    print("ğŸš€ ì„¹ì…˜ë³„ ìƒì„± ì‹œì‘...")
    
    sections = []
    
    # ê° ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸
    section_prompts = [
        {
            "name": "í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´",
            "prompt": f"{input_text}\n\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\n**í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´:**\n{{ì™„ì „í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ}}"
        },
        {
            "name": "ê´€ê³„ ë°ì´í„°", 
            "prompt": f"{input_text}\n\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë°ì´í„°ë² ì´ìŠ¤ ê´€ê³„ ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\n**ê´€ê³„ ë°ì´í„°:**\n[ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ]"
        },
        {
            "name": "ERD ë°ì´í„°",
            "prompt": f"{input_text}\n\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ERD ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\n**ERD ë°ì´í„°:**\n[ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ]"
        },
        {
            "name": "API ëª…ì„¸",
            "prompt": f"{input_text}\n\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ API ëª…ì„¸ ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\n**API ëª…ì„¸ ë°ì´í„°:**\n{{ì™„ì „í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ}}"
        }
    ]
    
    for i, section in enumerate(section_prompts):
        start_time = time.time()
        print(f"[{i+1}/4] {section['name']} ìƒì„± ì¤‘...")
        
        inputs = tokenizer(section['prompt'], return_tensors='pt', truncation=True, max_length=512)
        eos_token_id = tokenizer.convert_tokens_to_ids('<|eot_id|>')
        
        with torch.no_grad():
          outputs = model.generate(
              input_ids=inputs["input_ids"].to('cuda'),
              max_new_tokens=2048,        # 3072 â†’ 2048 (ì ë‹¹íˆ ì¤„ì´ê¸°)
              do_sample=False,            # True â†’ False (ê·¸ë¦¬ë””, ê°€ì¥ ë¹ ë¦„)
              use_cache=True,             # ìºì‹œ ì‚¬ìš©
              num_beams=1,               # ë¹” ì„œì¹˜ ë„ê¸°
              early_stopping=False,
              pad_token_id=tokenizer.eos_token_id,
              eos_token_id=None
          )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        section_content = response[len(section['prompt']):].strip()
        sections.append(section_content)
        
        elapsed = time.time() - start_time
        print(f"    âœ… ì™„ë£Œ: {len(section_content)} ë¬¸ì, {elapsed:.1f}ì´ˆ")
    
    # ëª¨ë“  ì„¹ì…˜ ê²°í•©
    full_response = input_text + "\n\nAssistant: " + "\n\n".join(sections)
    print(f"ğŸ‰ ì „ì²´ ì™„ë£Œ! ì´ {len(full_response)} ë¬¸ì ìƒì„±")
    return full_response

api = HfApi()
username="Min-kyu"
MODEL_NAME = "PJA_LLM_MODEL_8bit"

load_dotenv()

api_key = os.getenv('HUG_API_KEY')

# í•™ìŠµëœ ëª¨ë¸
tokenizer = AutoTokenizer.from_pretrained(f"{username}/{MODEL_NAME}")
model = AutoModelForCausalLM.from_pretrained(f"{username}/{MODEL_NAME}")

# ì¶”ë¡ 
input_text = """
ì´ í”„ë¡œì íŠ¸ëŠ” 3ì¸ì¹­ ì¹´ë“œê²Œì„ê¸°ë°˜ MMORPG ê²Œì„ ê°œë°œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì°½ì—…ì„ ëª©ì ìœ¼ë¡œ ê²Œì„ ë°ì´í„°ë¥¼ ëŒì–´ë‹¤ê°€ í”„ë¡œì íŠ¸ë¥¼ í•˜ë ¤ê³ í•©ë‹ˆë‹¤. ì£¼ì œëŠ” ê°„ë‹¨í•˜ê²Œ 3ì¸ì¹­ ì¹´ë“œê²Œì„ì„ mmorpgí˜•ì‹ìœ¼ë¡œ ë§Œë“œë ¤ê³  í•©ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë§Œë“¤ìƒê°ì´ëƒë©´ ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ê²Œ í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ê²Œì„ì„ ë§Œë“¤ê³ ì‹¶ìŠµë‹ˆë‹¤. ë˜í•œ, ìŠ¤í† ë¦¬ê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ê³  ì„ íƒí•˜ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ì— ë”°ë¼ì„œ ë“œë¡­ë˜ëŠ” ì¹´ë“œì˜ í˜•ì‹ì´ ë‹¬ëìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì´ ê²Œì„ì„ í†µí•´ì„œ ì‚¬ìš©ìê°€ ëœë¤ ê°€ì±  + ìˆœê°„ì ì¸ íŒë‹¨ìœ¼ë¡œ ì»¨íŠ¸ë¡¤í•˜ëŠ” ëŠ¥ë ¥ì´ ëŠ˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ê¸°íší•˜ì˜€ìŠµë‹ˆë‹¤.
"""

inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)
eos_token_id = tokenizer.convert_tokens_to_ids('<|eot_id|>')

with torch.no_grad():
    outputs = model.generate(
        input_ids=inputs["input_ids"].to('cuda'),
        max_new_tokens=500,
        eos_token_id=eos_token_id,
        do_sample=True,
        temperature=1.0,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(response.replace("', '", "',\n'").replace("}, {", "},\n{"))