### QLoRA Configuration for Consumer GPUs (RTX 4080/4090)
# 파일명: qlora_config.yaml
# 
# 사용법:
# python train_qlora.py --config qlora_config.yaml

model_name: "allganize/Llama-3-Alpha-Ko-8B-Instruct"
dataset_path: "."
max_seq_length: 1024                      # RTX 4080: 1024, RTX 4090: 2048
output_dir: "./llama-3.1-korean-8b-qlora-consumer"
report_to: "wandb"

# 저용량 GPU 최적화 학습률
learning_rate: 0.0002
lr_scheduler_type: "cosine"
num_train_epochs: 3
per_device_train_batch_size: 1            # RTX 4080: 1, RTX 4090: 2
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16           # 실효 배치 사이즈 16 유지
optim: "paged_adamw_8bit"                 # 메모리 효율적 optimizer

# 로깅 및 평가
logging_steps: 5
save_strategy: "steps"
save_steps: 25
eval_strategy: "steps"
eval_steps: 25
evaluation_strategy: "steps"

# 정규화
weight_decay: 0.01
max_grad_norm: 1.0
warmup_ratio: 0.03

# 메모리 최적화 (중요!)
bf16: true
tf32: true
gradient_checkpointing: true              # 필수!
dataloader_num_workers: 2                 # 낮게 설정
group_by_length: true
dataloader_pin_memory: false              # 메모리 부족 시 false

# 모델 저장
load_best_model_at_end: false             # 메모리 절약
metric_for_best_model: "eval_loss"
greater_is_better: false
save_total_limit: 1                       # 1개만 저장

# Early stopping
early_stopping_patience: 3

# 메모리 절약 설정
remove_unused_columns: true
dataloader_drop_last: true
seed: 42
data_seed: 42
fp16_full_eval: false

# QLoRA 활성화
use_qlora: yes

# 저용량 GPU 전용 설정
low_memory_mode: true