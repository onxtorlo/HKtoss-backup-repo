"""
QLoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©í•˜ì—¬ ë°°í¬ìš© ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
íŒŒì¼ëª…: merge_qlora.py

ì‹¤í–‰ ë°©ë²•:
python merge_qlora.py
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch
import gc
import os
from huggingface_hub import login
from dotenv import load_dotenv

load_dotenv()

def merge_qlora_adapter(
    base_model_name: str,
    adapter_path: str,
    output_path: str,
    push_to_hub: bool = False,
    hub_model_name: str = None
):
    """
    QLoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©
    
    Args:
        base_model_name: ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„/ê²½ë¡œ
        adapter_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ
        output_path: ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        push_to_hub: Hugging Face Hubì— í‘¸ì‹œ ì—¬ë¶€
        hub_model_name: Hubì— í‘¸ì‹œí•  ëª¨ë¸ ì´ë¦„
    """
    
    print("ğŸš€ QLoRA ì–´ëŒ‘í„° ë³‘í•© ì‹œì‘...")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()
    
    # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (bfloat16ìœ¼ë¡œ ë¡œë“œ)
    print(f"ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ: {base_model_name}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ...")
    tokenizer = AutoTokenizer.from_pretrained(adapter_path)
    
    # 3. LoRA ì–´ëŒ‘í„° ë¡œë“œ
    print(f"ğŸ”— LoRA ì–´ëŒ‘í„° ë¡œë“œ: {adapter_path}")
    model = PeftModel.from_pretrained(
        base_model,
        adapter_path,
        torch_dtype=torch.bfloat16
    )
    
    # 4. ì–´ëŒ‘í„° ë³‘í•©
    print("ğŸ”„ ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
    merged_model = model.merge_and_unload()
    
    # 5. ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
    print(f"ğŸ’¾ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥: {output_path}")
    os.makedirs(output_path, exist_ok=True)
    
    merged_model.save_pretrained(
        output_path,
        safe_serialization=True,  # SafeTensors í˜•ì‹ìœ¼ë¡œ ì €ì¥
        max_shard_size="5GB"      # í° ëª¨ë¸ì„ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„í• 
    )
    
    tokenizer.save_pretrained(output_path)
    
    # 6. ëª¨ë¸ ì¹´ë“œ ìƒì„±
    create_model_card(output_path, base_model_name, adapter_path)
    
    # 7. Hugging Face Hubì— í‘¸ì‹œ (ì„ íƒì‚¬í•­)
    if push_to_hub and hub_model_name:
        print(f"ğŸ“¤ Hugging Face Hubì— í‘¸ì‹œ: {hub_model_name}")
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv('HUG_API_KEY')
        if api_key:
            login(token=api_key)
            
            merged_model.push_to_hub(
                hub_model_name,
                private=False,  # ê³µê°œ ì„¤ì •
                commit_message="Upload merged QLoRA model"
            )
            tokenizer.push_to_hub(
                hub_model_name,
                commit_message="Upload tokenizer"
            )
            print(f"âœ… Hub ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{hub_model_name}")
        else:
            print("âš ï¸  HUG_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Hub ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # 8. ë©”ëª¨ë¦¬ ì •ë¦¬
    del merged_model, model, base_model
    torch.cuda.empty_cache()
    gc.collect()
    
    print("ğŸ‰ ì–´ëŒ‘í„° ë³‘í•© ì™„ë£Œ!")
    return output_path

def create_model_card(output_path: str, base_model: str, adapter_path: str):
    """ëª¨ë¸ ì¹´ë“œ ìƒì„±"""
    model_card = f"""
---
license: apache-2.0
base_model: {base_model}
tags:
- llama
- qlora
- korean
- project-analysis
- fine-tuned
language:
- ko
- en
pipeline_tag: text-generation
---

# Korean Project Analysis Model (QLoRA Fine-tuned)

ì´ ëª¨ë¸ì€ {base_model}ì„ QLoRA ë°©ì‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•œ í•œêµ­ì–´ í”„ë¡œì íŠ¸ ë¶„ì„ ì „ë¬¸ ëª¨ë¸ì…ë‹ˆë‹¤.

## ëª¨ë¸ ì •ë³´

- **ë² ì´ìŠ¤ ëª¨ë¸**: {base_model}
- **íŒŒì¸íŠœë‹ ë°©ë²•**: QLoRA (4-bit quantization + LoRA adapters)
- **ì–´ëŒ‘í„° ê²½ë¡œ**: {adapter_path}
- **ì£¼ìš” ê¸°ëŠ¥**: í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ ë¶„ì„, ERD ì„¤ê³„, API ëª…ì„¸ ìƒì„±

## ì‚¬ìš©ë²•

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "your-username/your-model-name"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# ì¶”ë¡  ì˜ˆì‹œ
messages = [
    {{"role": "system", "content": "ë‹¹ì‹ ì€ í”„ë¡œì íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤..."}},
    {{"role": "user", "content": "ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼ì„ ê°œë°œí•˜ê³  ì‹¶ìŠµë‹ˆë‹¤."}}
]

input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=4096,
        temperature=0.1,
        do_sample=True
    )

response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
print(response)
```

## ì¶œë ¥ í˜•ì‹

ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤:

1. **í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´**: ì¢…í•© ë¶„ì„ ë° ê¸°ìˆ  ìŠ¤íƒ
2. **ê´€ê³„ ë°ì´í„°**: ë°ì´í„°ë² ì´ìŠ¤ ê´€ê³„ ì •ì˜
3. **ERD ë°ì´í„°**: ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ì„¤ê³„
4. **API ëª…ì„¸ ë°ì´í„°**: OpenAPI 3.0 í‘œì¤€ ë¬¸ì„œ

## ì„±ëŠ¥ íŠ¹ì§•

- QLoRA ê¸°ë°˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- í•œêµ­ì–´ í”„ë¡œì íŠ¸ ë¶„ì„ì— íŠ¹í™”
- ì‹¤ë¬´ ìˆ˜ì¤€ì˜ ê¸°ìˆ  ë¬¸ì„œ ìƒì„±
- JSON í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ì¶œë ¥

## ë¼ì´ì„ ìŠ¤

Apache 2.0 License

## í•™ìŠµ ë°ì´í„°

í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ì™€ ìƒì‘í•˜ëŠ” ê¸°ìˆ  ë¬¸ì„œ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.
"""

    with open(os.path.join(output_path, "README.md"), "w", encoding="utf-8") as f:
        f.write(model_card.strip())

def test_merged_model(model_path: str, test_prompt: str = None):
    """ë³‘í•©ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ§ª ë³‘í•©ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸: {model_path}")
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    if test_prompt is None:
        test_prompt = "ëª¨ë°”ì¼ ì•± ê¸°ë°˜ ë°°ë‹¬ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ ì£¼ë¬¸ ì²˜ë¦¬ì™€ GPS ì¶”ì  ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    messages = [
        {
            "role": "system", 
            "content": "ë‹¹ì‹ ì€ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”í•˜ì—¬ êµ¬ì²´ì ì¸ ê°œë°œ ê³„íšì„ ì œì‹œí•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
        },
        {
            "role": "user", 
            "content": test_prompt
        }
    ]
    
    # ì¶”ë¡  ì‹¤í–‰
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    print("ğŸ¤– ìƒì„± ì¤‘...")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    
    print(f"ğŸ“ ì…ë ¥: {test_prompt}")
    print(f"ğŸ¯ ì¶œë ¥ (ì²˜ìŒ 500ì): {response[:500]}...")
    
    # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
    quality_indicators = ["project_summary", "ERD", "API"]
    found_indicators = sum(1 for indicator in quality_indicators if indicator in response)
    
    if found_indicators >= 2:
        print(f"âœ… í’ˆì§ˆ ì²´í¬: í†µê³¼ ({found_indicators}/3 ì§€í‘œ í¬í•¨)")
    else:
        print(f"âš ï¸  í’ˆì§ˆ ì²´í¬: ê²€í†  í•„ìš” ({found_indicators}/3 ì§€í‘œ í¬í•¨)")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    torch.cuda.empty_cache()

if __name__ == "__main__":
    # ì„¤ì •
    BASE_MODEL = "allganize/Llama-3-Alpha-Ko-8B-Instruct"
    ADAPTER_PATH = "./llama-3.1-korean-8b-qlora/lora_adapter"
    OUTPUT_PATH = "./llama-3.1-korean-8b-merged"
    HUB_MODEL_NAME = "your-username/llama-3.1-korean-project-analyzer"  # ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë³€ê²½
    
    # ì–´ëŒ‘í„° ë³‘í•© ì‹¤í–‰
    merged_path = merge_qlora_adapter(
        base_model_name=BASE_MODEL,
        adapter_path=ADAPTER_PATH,
        output_path=OUTPUT_PATH,
        push_to_hub=False,  # Trueë¡œ ë³€ê²½í•˜ë©´ Hubì— ì—…ë¡œë“œ
        hub_model_name=HUB_MODEL_NAME
    )
    
    # ë³‘í•©ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    test_merged_model(merged_path)
    
    print("\nğŸ“‹ ì‚¬ìš©ë²•:")
    print(f"1. ë¡œì»¬ ì‚¬ìš©: python inference.py --model_path {OUTPUT_PATH}")
    print(f"2. Hub ì—…ë¡œë“œ í›„: python inference.py --model_name {HUB_MODEL_NAME}")