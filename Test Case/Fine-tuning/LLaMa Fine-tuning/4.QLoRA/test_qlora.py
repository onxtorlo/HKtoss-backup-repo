"""
QLoRA íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
íŒŒì¼ëª…: test_qlora.py

ì‹¤í–‰ ë°©ë²•:
python test_qlora.py
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch
import json
from datasets import load_dataset

def load_qlora_model(base_model_path, adapter_path):
    """QLoRA ì–´ëŒ‘í„°ê°€ ì ìš©ëœ ëª¨ë¸ ë¡œë“œ"""
    
    # 4bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)
    print("ğŸ“¦ Loading base model with 4-bit quantization...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(adapter_path)
    
    # QLoRA ì–´ëŒ‘í„° ì ìš©
    print("ğŸ”— Loading LoRA adapter...")
    model = PeftModel.from_pretrained(
        base_model, 
        adapter_path,
        torch_dtype=torch.bfloat16,
    )
    
    # ì¶”ë¡ ì„ ìœ„í•´ ì–´ëŒ‘í„° ë³‘í•© (ì„ íƒì‚¬í•­)
    # model = model.merge_and_unload()  # ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´ ì‚¬ìš©
    
    return model, tokenizer

def test_qlora_model():
    """QLoRA íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    # ê²½ë¡œ ì„¤ì •
    base_model_path = "allganize/Llama-3-Alpha-Ko-8B-Instruct"
    adapter_path = "./llama-3.1-korean-8b-qlora/lora_adapter"
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_qlora_model(base_model_path, adapter_path)
    
    # ì¶”ë¡  ìµœì í™” ì„¤ì •
    model.eval()
    torch.backends.cuda.matmul.allow_tf32 = True
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_dataset = load_dataset("json", data_files="../data/test_dataset.json", split="train")
    print(f"ğŸ“Š Test ë°ì´í„° ê°œìˆ˜: {len(test_dataset)}")
    
    # ëª‡ ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    num_samples = min(3, len(test_dataset))
    
    for i in range(num_samples):
        sample = test_dataset[i]
        messages = sample['messages']
        
        # ì‹œìŠ¤í…œ + ìœ ì € í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
        test_messages = [
            messages[0],  # system
            messages[1]   # user
        ]
        
        # ì •ë‹µ (ê¸°ëŒ€ ì¶œë ¥)
        expected_output = messages[2]['content']
        
        print(f"\\n{'='*60}")
        print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i+1}:")
        print(f"{'='*60}")
        print(f"ì…ë ¥: {messages[1]['content'][:150]}...")
        
        # ëª¨ë¸ ì¶”ë¡ 
        formatted_prompt = tokenizer.apply_chat_template(
            test_messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì¦ˆ ë° GPUë¡œ ì „ì†¡
        inputs = tokenizer(
            formatted_prompt, 
            return_tensors="pt",
            max_length=2048,
            truncation=True
        ).to(model.device)
        
        print("ğŸ¤– ìƒì„± ì¤‘...")
        
        # ìƒì„± ì„¤ì •
        generation_config = {
            "max_new_tokens": 4096,
            "min_new_tokens": 2000,
            "do_sample": True,              # QLoRAëŠ” samplingì´ ë” ì¢‹ì„ ìˆ˜ ìˆìŒ
            "temperature": 0.1,             # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í™•ë³´
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "pad_token_id": tokenizer.eos_token_id,
            "use_cache": True,
            "early_stopping": False,
        }
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **generation_config
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_output = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        print(f"\\nğŸ“ ê¸°ëŒ€ ì¶œë ¥ (ì²˜ìŒ 200ì):")
        print(f"{expected_output[:200]}...")
        print(f"\\nğŸ¯ ì‹¤ì œ ì¶œë ¥ (ì²˜ìŒ 200ì):")
        print(f"{generated_output[:200]}...")
        
        # í’ˆì§ˆ í‰ê°€
        required_keywords = ["project_summary", "relationships_data", "ERD_data", "API_specs_data"]
        found_keywords = sum(1 for kw in required_keywords if kw in generated_output)
        
        if found_keywords >= 3:
            print(f"\\nâœ… êµ¬ì¡°ì  ì¼ê´€ì„±: ì¢‹ìŒ ({found_keywords}/4 í‚¤ì›Œë“œ í¬í•¨)")
        else:
            print(f"\\nâŒ êµ¬ì¡°ì  ì¼ê´€ì„±: ë¶€ì¡± ({found_keywords}/4 í‚¤ì›Œë“œ í¬í•¨)")
            
        print(f"ğŸ“ ì¶œë ¥ ê¸¸ì´: {len(generated_output)}ì")
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            # ì¶œë ¥ì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„
            if "**í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´:**" in generated_output:
                json_start = generated_output.find("{'project_summary'")
                if json_start != -1:
                    # ì²« ë²ˆì§¸ JSON ë¸”ë¡ë§Œ ê²€ì¦
                    json_part = generated_output[json_start:json_start+500]
                    print("ğŸ“‹ JSON í˜•ì‹ ì²´í¬: ì‹œì‘ ë¶€ë¶„ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì…ë‹ˆë‹¤")
                else:
                    print("âš ï¸  JSON í˜•ì‹ ì²´í¬: project_summaryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            else:
                print("âš ï¸  JSON í˜•ì‹ ì²´í¬: ì˜ˆìƒëœ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        
        print("-" * 60)
    
    print("\\nğŸ‰ QLoRA ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    torch.cuda.empty_cache()

def compare_model_sizes():
    """ëª¨ë¸ í¬ê¸° ë¹„êµ"""
    print("ğŸ“Š ëª¨ë¸ í¬ê¸° ë¹„êµ:")
    print("â€¢ Full Fine-tuning: ~16GB (ì „ì²´ ëª¨ë¸ íŒŒë¼ë¯¸í„°)")
    print("â€¢ QLoRA: ~4GB (4bit ì–‘ìí™”) + ~100-200MB (LoRA adapter)")
    print("â€¢ ë©”ëª¨ë¦¬ ì ˆì•½: ì•½ 75% ê°ì†Œ")
    print("\\nâš¡ QLoRA ì¥ì :")
    print("â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ")
    print("â€¢ í•™ìŠµ ì†ë„ í–¥ìƒ") 
    print("â€¢ ì—¬ëŸ¬ íƒœìŠ¤í¬ìš© ì–´ëŒ‘í„° ê´€ë¦¬ ìš©ì´")
    print("â€¢ ë² ì´ìŠ¤ ëª¨ë¸ ì¬ì‚¬ìš© ê°€ëŠ¥")

if __name__ == "__main__":
    compare_model_sizes()
    print("\\n" + "="*60)
    test_qlora_model()