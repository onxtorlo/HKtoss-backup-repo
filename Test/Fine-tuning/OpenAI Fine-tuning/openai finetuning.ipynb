{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9770debf",
   "metadata": {},
   "source": [
    "## GPT-4o-mini Fine-tuning\n",
    "- base model : gpt-4o-mini-2024-07-18\n",
    "- output model : ft:gpt-4o-mini-2024-07-18:test::BebIPMSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 라이브러리 설치\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12067399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# .env 파일 로드 (있다면)\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7ca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 OpenAI GPT-4o-mini 파인튜닝\n",
      "========================================\n",
      "✅ API 키 설정 완료: sk-proj-...\n",
      "✅ OpenAI 연결 성공! (사용 가능한 모델: 75개)\n",
      "🚀 OpenAI 파인튜닝 시작!\n",
      "==================================================\n",
      "\n",
      "📁 데이터 준비: finetuning_dataset.json\n",
      "📊 로드된 샘플 수: 100\n",
      "✅ 변환 완료: training_data.jsonl (100개 샘플)\n",
      "\n",
      "📤 파일 업로드 중...\n",
      "✅ 업로드 완료 - 파일 ID: file-Y3vVsmEefYHSc8cL6BditE\n",
      "\n",
      "🎯 파인튜닝 작업 생성 중...\n",
      "✅ 작업 생성 완료 - Job ID: ftjob-2J88orY1TKFzxvqDK4SQZg51\n",
      "📊 초기 상태: validating_files\n",
      "\n",
      "👀 파인튜닝 상태 모니터링...\n",
      "🔗 상세 모니터링: https://platform.openai.com/finetune/ftjob-2J88orY1TKFzxvqDK4SQZg51\n",
      "📊 상태 확인 1: validating_files\n",
      "⏳ 진행 중... 30초 후 재확인\n",
      "📊 상태 확인 2: validating_files\n",
      "⏳ 진행 중... 30초 후 재확인\n",
      "📊 상태 확인 3: validating_files\n",
      "⏳ 진행 중... 30초 후 재확인\n",
      "📊 상태 확인 4: validating_files\n",
      "⏳ 진행 중... 30초 후 재확인\n",
      "📊 상태 확인 5: running\n",
      "⏳ 계속 진행 중입니다. 웹 콘솔에서 확인하세요.\n",
      "\n",
      "📋 파인튜닝 정보:\n",
      "  - Job ID: ftjob-2J88orY1TKFzxvqDK4SQZg51\n",
      "  - 모니터링 명령어: client.fine_tuning.jobs.retrieve('ftjob-2J88orY1TKFzxvqDK4SQZg51')\n",
      "  - 웹 콘솔: https://platform.openai.com/finetune\n",
      "\n",
      "⏳ 파인튜닝이 진행 중입니다\n",
      "📋 Job ID: ftjob-2J88orY1TKFzxvqDK4SQZg51\n",
      "🔍 상태 확인: check_finetuning_status('ftjob-2J88orY1TKFzxvqDK4SQZg51')\n"
     ]
    }
   ],
   "source": [
    "# 환경변수 및 API 키 설정\n",
    "def setup_api_key():\n",
    "    try:\n",
    "        dotenv.load_dotenv()\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY가 .env 파일이나 환경변수에 설정되지 않았습니다.\")\n",
    "        \n",
    "        # API 키 형식 간단 검증 (sk-로 시작하는지)\n",
    "        if not api_key.startswith('sk-'):\n",
    "            raise ValueError(\"올바른 OpenAI API 키 형식이 아닙니다.\")\n",
    "            \n",
    "        return api_key\n",
    "    \n",
    "      except Exception as e:\n",
    "        print(f\"API 키 설정 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "# Finetuning Class\n",
    "class FineTuner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.api_key = setup_api_key() # API 키 설정\n",
    "        self.client = openai.OpenAI(api_key=self.api_key) # OpenAI 클라이언트 생성\n",
    "        self.model_name = \"gpt-4o-mini-2024-07-18\" # 모델 설정\n",
    "        self.test_connection() # 연결 테스트\n",
    "    \n",
    "    # 모델 연결 테스트트\n",
    "    def test_connection(self):\n",
    "        try:\n",
    "            models = self.client.models.list()\n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    def prepare_data(self, input_file: str = \"finetuning_dataset.json\"):\n",
    "        \"\"\"데이터 준비\"\"\"\n",
    "        print(f\"\\n📁 데이터 준비: {input_file}\")\n",
    "        \n",
    "        # 파일 존재 확인\n",
    "        if not Path(input_file).exists():\n",
    "            raise FileNotFoundError(f\"❌ 파일을 찾을 수 없습니다: {input_file}\")\n",
    "        \n",
    "        # 데이터 로드\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"📊 로드된 샘플 수: {len(data)}\")\n",
    "        \n",
    "        # OpenAI 형식으로 변환\n",
    "        converted_data = []\n",
    "        \n",
    "        for item in data:\n",
    "            if 'messages' in item:\n",
    "                # 이미 올바른 형식\n",
    "                converted_data.append(item)\n",
    "            else:\n",
    "                # instruction/output을 messages로 변환\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\", \n",
    "                        \"content\": \"당신은 프로젝트 아이디어를 체계적으로 분석하고 구조화하여 구체적인 개발 계획을 제시하는 전문 AI 어시스턴트입니다.\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": item.get('instruction', '')},\n",
    "                    {\"role\": \"assistant\", \"content\": item.get('output', '')}\n",
    "                ]\n",
    "                converted_data.append({\"messages\": messages})\n",
    "        \n",
    "        # JSONL 파일 생성\n",
    "        output_file = \"training_data.jsonl\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in converted_data:\n",
    "                json.dump(item, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "        \n",
    "        print(f\"✅ 변환 완료: {output_file} ({len(converted_data)}개 샘플)\")\n",
    "        return output_file\n",
    "    \n",
    "    def run_finetuning(self, input_file: str = \"finetuning_dataset.json\"):\n",
    "        \"\"\"파인튜닝 전체 프로세스 실행\"\"\"\n",
    "        try:\n",
    "            print(\"🚀 OpenAI 파인튜닝 시작!\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # 1단계: 데이터 준비\n",
    "            training_file = self.prepare_data(input_file)\n",
    "            \n",
    "            # 2단계: 파일 업로드\n",
    "            print(f\"\\n📤 파일 업로드 중...\")\n",
    "            with open(training_file, 'rb') as f:\n",
    "                file_response = self.client.files.create(\n",
    "                    file=f,\n",
    "                    purpose='fine-tune'\n",
    "                )\n",
    "            \n",
    "            file_id = file_response.id\n",
    "            print(f\"✅ 업로드 완료 - 파일 ID: {file_id}\")\n",
    "            \n",
    "            # 3단계: 파인튜닝 작업 생성\n",
    "            print(f\"\\n🎯 파인튜닝 작업 생성 중...\")\n",
    "            job_response = self.client.fine_tuning.jobs.create(\n",
    "                training_file=file_id,\n",
    "                model=self.model_name,\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 3,\n",
    "                    \"batch_size\": \"auto\",\n",
    "                    \"learning_rate_multiplier\": \"auto\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            job_id = job_response.id\n",
    "            print(f\"✅ 작업 생성 완료 - Job ID: {job_id}\")\n",
    "            print(f\"📊 초기 상태: {job_response.status}\")\n",
    "            \n",
    "            # 4단계: 상태 모니터링 (간단 버전)\n",
    "            print(f\"\\n👀 파인튜닝 상태 모니터링...\")\n",
    "            print(f\"🔗 상세 모니터링: https://platform.openai.com/finetune/{job_id}\")\n",
    "            \n",
    "            # 처음 몇 번만 확인\n",
    "            for i in range(5):\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                status = job.status\n",
    "                \n",
    "                print(f\"📊 상태 확인 {i+1}: {status}\")\n",
    "                \n",
    "                if status == \"succeeded\":\n",
    "                    print(f\"🎉 파인튜닝 완료!\")\n",
    "                    print(f\"🤖 모델 ID: {job.fine_tuned_model}\")\n",
    "                    return job.fine_tuned_model\n",
    "                \n",
    "                elif status == \"failed\":\n",
    "                    print(f\"❌ 파인튜닝 실패: {job.error}\")\n",
    "                    return None\n",
    "                \n",
    "                elif status in [\"validating_files\", \"queued\", \"running\"]:\n",
    "                    if i < 4:  # 마지막이 아니면\n",
    "                        print(f\"⏳ 진행 중... 30초 후 재확인\")\n",
    "                        time.sleep(30)\n",
    "                    else:\n",
    "                        print(f\"⏳ 계속 진행 중입니다. 웹 콘솔에서 확인하세요.\")\n",
    "                        break\n",
    "            \n",
    "            print(f\"\\n📋 파인튜닝 정보:\")\n",
    "            print(f\"  - Job ID: {job_id}\")\n",
    "            print(f\"  - 모니터링 명령어: client.fine_tuning.jobs.retrieve('{job_id}')\")\n",
    "            print(f\"  - 웹 콘솔: https://platform.openai.com/finetune\")\n",
    "            \n",
    "            return job_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파인튜닝 실패: {e}\")\n",
    "            raise\n",
    "\n",
    "def quick_test_model(model_id: str, prompt: str):\n",
    "    \"\"\"파인튜닝된 모델 빠른 테스트\"\"\"\n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 프로젝트 분석 전문가입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 테스트 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_finetuning_status(job_id: str):\n",
    "    \"\"\"파인튜닝 상태 확인\"\"\"\n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    try:\n",
    "        job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "        \n",
    "        print(f\"📊 파인튜닝 상태 정보:\")\n",
    "        print(f\"  - Job ID: {job.id}\")\n",
    "        print(f\"  - 상태: {job.status}\")\n",
    "        print(f\"  - 모델: {job.model}\")\n",
    "        print(f\"  - 완성된 모델: {job.fine_tuned_model or 'N/A'}\")\n",
    "        print(f\"  - 생성일: {job.created_at}\")\n",
    "        \n",
    "        if job.status == \"succeeded\":\n",
    "            print(f\"✅ 파인튜닝 완료! 모델 사용 가능\")\n",
    "            return job.fine_tuned_model\n",
    "        elif job.status == \"failed\":\n",
    "            print(f\"❌ 파인튜닝 실패: {job.error}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"⏳ 아직 진행 중입니다\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 상태 확인 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 메인 실행 함수\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    print(\"🎯 OpenAI GPT-4o-mini 파인튜닝\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    try:\n",
    "        # 파인튜닝 실행\n",
    "        finetuner = FineTuner()\n",
    "        result = finetuner.run_finetuning(\"finetuning_dataset.json\")\n",
    "        \n",
    "        if result:\n",
    "            if result.startswith(\"ft:\"):\n",
    "                # 완성된 모델 ID\n",
    "                print(f\"\\n🎉 파인튜닝 완료!\")\n",
    "                print(f\"🤖 모델 ID: {result}\")\n",
    "                \n",
    "                # 간단 테스트\n",
    "                test_prompt = \"간단한 웹사이트를 만들고 싶습니다.\"\n",
    "                print(f\"\\n🧪 테스트: {test_prompt}\")\n",
    "                test_result = quick_test_model(result, test_prompt)\n",
    "                if test_result:\n",
    "                    print(f\"🤖 응답: {test_result[:200]}...\")\n",
    "                \n",
    "            else:\n",
    "                # Job ID\n",
    "                print(f\"\\n⏳ 파인튜닝이 진행 중입니다\")\n",
    "                print(f\"📋 Job ID: {result}\")\n",
    "                print(f\"🔍 상태 확인: check_finetuning_status('{result}')\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 바로 실행\n",
    "    main()\n",
    "    \n",
    "    # 또는 개별 함수 사용\n",
    "    # finetuner = QuickFineTuner()\n",
    "    # job_id = finetuner.run_finetuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba3b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 상태: running\n",
      "⚠️ 과적합 위험 - 조기 종료 고려\n"
     ]
    }
   ],
   "source": [
    "# 현재 훈련 상태 확인\n",
    "import openai\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Job 상태 확인\n",
    "job = client.fine_tuning.jobs.retrieve(\"ftjob-2J88orY1TKFzxvqDK4SQZg51\")\n",
    "print(f\"현재 상태: {job.status}\")\n",
    "\n",
    "# 만약 아직 running이면 취소 고려\n",
    "if job.status == \"running\":\n",
    "    print(\"⚠️ 과적합 위험 - 조기 종료 고려\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17b1ca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론입니다! 간단한 웹사이트를 만들기 위한 기본적인 HTML 및 CSS 코드를 제공하겠습니다. 이 웹사이트는 홈페이지, 소개, 연락처 페이지로 구성됩니다.\n",
      "\n",
      "### 1. 기본 HTML 구조\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"ko\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>간단한 웹사이트</title>\n",
      "    <link rel=\"stylesheet\" href=\"styles.css\">\n",
      "</head>\n",
      "<body>\n",
      "    <header>\n",
      "        <h1>나의 웹사이트</h1>\n",
      "        <nav>\n",
      "            <ul>\n",
      "                <li><a href=\"#home\">홈</a></li>\n",
      "                <li><a href=\"#about\">소개</a></li>\n",
      "                <li><a href=\"#contact\">연락처</a></li>\n",
      "            </ul>\n",
      "        </nav>\n",
      "    </header>\n",
      "\n",
      "    <main>\n",
      "        <section id=\"home\">\n",
      "            <h2>홈</h2>\n",
      "            <p>환영합니다! 이것은 간단한 웹사이트입니다.</p>\n",
      "        </section>\n",
      "\n",
      "        <section id=\"about\">\n",
      "            <h2>소개</h2>\n",
      "            <p>이 웹사이트는 HTML 및 CSS로 만들어졌습니다.</p>\n",
      "        </section>\n",
      "\n",
      "        <section id=\"contact\">\n",
      "            <h2>연락\n"
     ]
    }
   ],
   "source": [
    "model_id = \"ft:gpt-4o-mini-2024-07-18:test::BebIPMSD\"  # 완료된 모델 ID\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 프로젝트 분석 전문가입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"간단한 웹사이트 만들어줘\"}  # 짧고 간단한 질문\n",
    "    ],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92fa23da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 고급 테스트 시작...\n",
      "🔬 고급 과적합 테스트 시작\n",
      "==================================================\n",
      "\n",
      "1. 완전히 다른 도메인\n",
      "📝 질문: 우리 강아지가 아픈데 어떻게 해야 할까요?\n",
      "⚠️ 과적합 징후: 프로젝트 분석으로 답변하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: 강아지가 아프면 걱정이 많으시겠어요. 다음과 같은 단계를 고려해 보세요:\n",
      "\n",
      "1. **증상 관찰**: 강아지가 어떤 증상을 보이는지 자세히 관찰하세요. 예를 들어, 식욕 부진, 구토, 설사, 기침, lethargy(무기력) 등의 증상이 있는지 체크해 보세요.\n",
      "\n",
      "2. **수의사 방문**: 증상이 심각하거나 지속된다면, 즉시 수의사에게 데려가세요. 전문가의 진단과 치료가 필요합니다.\n",
      "\n",
      "3. **안정된 환경 제공**: 아픈 강아지가 편안하게 쉴 수 있도록 조용하고 안정된 환경을 만들어 주세요.\n",
      "\n",
      "4. **수분 섭취**: 충분한 수분을 섭취할 수 있도록 도와주세요. 물을 잘 마시는지 확인하고, 필요하다면 물기를 줄 수 있는 음식을 제공하세요.\n",
      "\n",
      "5. **약물 복용**: 수의사가 처방한 약물이 있다면, 정확한 용량과 주기를 지켜서 복용시켜 주세요.\n",
      "\n",
      "강아지의 건강이 빨리 회복되길 바랍니다!\n",
      "✅ 정상적인 응답\n",
      "\n",
      "2. 창의적 요청\n",
      "📝 질문: 재미있는 농담 하나 해주세요\n",
      "⚠️ 과적합 징후: 기술적 분석으로 답변하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: 물론이죠! 다음 농담 어떠세요?\n",
      "\n",
      "왜 컴퓨터는 바다에 빠지지 않나요?\n",
      "\n",
      "바다에 가면 항상 \"파일\"을 찾기 때문이에요!\n",
      "✅ 정상적인 응답\n",
      "\n",
      "3. 모호한 비즈니스 질문\n",
      "📝 질문: 우리 회사 매출을 늘리려면?\n",
      "⚠️ 과적합 징후: 구체적 정보 없이 JSON 생성하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: 회사의 매출을 늘리기 위해서는 다양한 전략과 접근 방식이 필요합니다. 다음은 몇 가지 효과적인 방법입니다:\n",
      "\n",
      "1. **고객 이해 및 타겟팅**:\n",
      "   - 고객 데이터를 분석하여 타겟 고객을 정의하고, 그들의 필요와 선호를 이해합니다.\n",
      "   - 세분화된 마케팅 전략을 통해 특정 고객 그룹에 맞춘 캠페인을 진행합니다.\n",
      "\n",
      "2. **제품 및 서비스 개선**:\n",
      "   - 고객 피드백을 반영하여 제품이나 서비스를 개선합니다.\n",
      "   - 새로운 제품 라인이나 서비스를 개발하여 시장의 요구에 부응합니다.\n",
      "\n",
      "3. **온라인 존재 강화**:\n",
      "   - 웹사이트와 소셜 미디어 플랫폼을 통해 브랜드 인지도를 높입니다.\n",
      "   - SEO(검색 엔진 최적화)와 콘텐츠 마케팅을 통해 유입 트래픽을 증가시킵니다.\n",
      "\n",
      "4. **판매 채널 다각화**:\n",
      "   - 오프라인 판매뿐만 아니라 온라인 쇼핑몰, 마켓플레이스 등을 통해 판매 채널을 확장합니다.\n",
      "   - 파트너십이나 제휴를 통해 새로운 유통 경로를 확보합니다.\n",
      "\n",
      "5. **고객 관계 관리(CRM)**:\n",
      "   - 고객과의 관계를 관리하고 유지하기 위해 CRM 시스템을 도입합니다.\n",
      "   - 충성도 프로그램을 통해 기존 고객의 재구매를 유도합니다.\n",
      "\n",
      "6. **마\n",
      "✅ 정상적인 응답\n",
      "\n",
      "4. 개인적 조언\n",
      "📝 질문: 요즘 스트레스가 많은데 어떻게 해야 할까요?\n",
      "⚠️ 과적합 징후: 프로젝트 구조로 답변하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: 스트레스를 줄이기 위한 몇 가지 방법을 소개할게요:\n",
      "\n",
      "1. **운동**: 규칙적인 운동은 스트레스 해소에 큰 도움이 됩니다. 걷기, 요가, 수영 등 자신이 좋아하는 활동을 해보세요.\n",
      "\n",
      "2. **명상 및 호흡법**: 명상이나 깊은 호흡은 마음을 안정시키고 스트레스를 줄이는 데 효과적입니다. 하루에 몇 분씩 시간을 내어 집중해보세요.\n",
      "\n",
      "3. **취미 활동**: 좋아하는 취미에 시간을 투자하는 것도 스트레스 해소에 좋습니다. 그림 그리기, 음악 듣기, 독서 등 자신이 즐기는 활동을 해보세요.\n",
      "\n",
      "4. **사회적 연결**: 친구나 가족과의 대화는 스트레스를 줄이는 데 도움이 됩니다. 감정을 나누고 지지받는 것이 중요합니다.\n",
      "\n",
      "5. **휴식**: 충분한 수면과 휴식은 스트레스 관리에 필수적입니다. 규칙적인 수면 패턴을 유지하고, 필요할 때는 짧은 휴식을 취하세요.\n",
      "\n",
      "6. **시간 관리**: 할 일을 우선순위에 따라 정리하고, 과도한 부담을 피하는 것이 중요합니다. 계획적으로 시간을 관리해보세요.\n",
      "\n",
      "7. **전문가 도움**: 만약 스트레스가 심각하다면 전문가의 도움을 받는 것도 좋은\n",
      "✅ 정상적인 응답\n",
      "\n",
      "5. 단순 팩트 질문\n",
      "📝 질문: 서울의 인구는?\n",
      "⚠️ 과적합 징후: 복잡한 API 명세로 답변하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: 2023년 기준으로 서울의 인구는 약 9백만 명 정도입니다. 그러나 인구는 변동이 있을 수 있으니, 최신 정보를 확인하는 것이 좋습니다. 서울특별시의 공식 웹사이트나 통계청의 자료를 참고하면 더 정확한 정보를 얻을 수 있습니다.\n",
      "✅ 정상적인 응답\n",
      "\n",
      "6. 영어 질문\n",
      "📝 질문: What's the weather like today?\n",
      "⚠️ 과적합 징후: 한국어 기술 문서로 답변하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: I'm unable to provide real-time information such as current weather conditions. You can check a weather website or app for the most accurate and up-to-date information.\n",
      "✅ 정상적인 응답\n",
      "\n",
      "7. 감정적 표현\n",
      "📝 질문: 오늘 정말 기분이 좋아요!\n",
      "⚠️ 과적합 징후: 데이터베이스 설계로 응답하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: 정말 기분이 좋으시다니 저도 기쁩니다! 어떤 특별한 일이 있었나요?\n",
      "✅ 정상적인 응답\n",
      "\n",
      "8. 수학 문제\n",
      "📝 질문: 2 + 2는?\n",
      "⚠️ 과적합 징후: 프로젝트 분석으로 답변하면 과적합\n",
      "------------------------------\n",
      "🤖 응답: 2 + 2는 4입니다.\n",
      "✅ 정상적인 응답\n",
      "\n",
      "==================================================\n",
      "🎯 고급 테스트 결과\n",
      "==================================================\n",
      "📊 과적합 징후: 0/8 (0.0%)\n",
      "🎉 훌륭함! 과적합 없이 잘 훈련됨\n",
      "✅ 모델 사용 가능\n",
      "\n",
      "==================================================\n",
      "🔄 원본 모델과 비교 테스트\n",
      "같은 질문을 원본 gpt-4o-mini에도 물어보세요:\n",
      "📝 테스트 질문: 간단한 웹사이트 만들어줘\n",
      "\n",
      "💡 비교 포인트:\n",
      "1. 응답 길이 차이\n",
      "2. 전문 용어 사용 빈도\n",
      "3. 구조화 정도\n",
      "4. 실용성\n",
      "\n",
      "🤖 원본 gpt-4o-mini 응답:\n",
      "물론입니다! 간단한 HTML 웹사이트의 기본 구조를 제공해드리겠습니다. 아래는 기본적인 HTML, CSS를 포함한 웹사이트의 예제 코드입니다.\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"ko\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=dev...\n",
      "\n",
      "🎯 최종 평가 및 권장사항\n",
      "========================================\n",
      "\n",
      "✅ 훌륭합니다! 현재 모델 사용을 권장합니다.\n",
      "💡 다음 번 파인튜닝에서는:\n",
      "  - 더 다양한 도메인 데이터 추가\n",
      "  - validation 데이터 포함 고려\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "더 까다로운 과적합 테스트\n",
    "첫 번째 테스트가 괜찮았다면 이것들도 해보세요\n",
    "\"\"\"\n",
    "\n",
    "import openai\n",
    "\n",
    "def advanced_overfitting_test(model_id: str):\n",
    "    \"\"\"고급 과적합 테스트\"\"\"\n",
    "    \n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    challenging_tests = [\n",
    "        {\n",
    "            \"name\": \"완전히 다른 도메인\",\n",
    "            \"prompt\": \"우리 강아지가 아픈데 어떻게 해야 할까요?\",\n",
    "            \"warning\": \"프로젝트 분석으로 답변하면 과적합\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"창의적 요청\",\n",
    "            \"prompt\": \"재미있는 농담 하나 해주세요\",\n",
    "            \"warning\": \"기술적 분석으로 답변하면 과적합\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"모호한 비즈니스 질문\",\n",
    "            \"prompt\": \"우리 회사 매출을 늘리려면?\",\n",
    "            \"warning\": \"구체적 정보 없이 JSON 생성하면 과적합\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"개인적 조언\",\n",
    "            \"prompt\": \"요즘 스트레스가 많은데 어떻게 해야 할까요?\",\n",
    "            \"warning\": \"프로젝트 구조로 답변하면 과적합\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"단순 팩트 질문\",\n",
    "            \"prompt\": \"서울의 인구는?\",\n",
    "            \"warning\": \"복잡한 API 명세로 답변하면 과적합\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"영어 질문\",\n",
    "            \"prompt\": \"What's the weather like today?\",\n",
    "            \"warning\": \"한국어 기술 문서로 답변하면 과적합\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"감정적 표현\",\n",
    "            \"prompt\": \"오늘 정말 기분이 좋아요!\",\n",
    "            \"warning\": \"데이터베이스 설계로 응답하면 과적합\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"수학 문제\",\n",
    "            \"prompt\": \"2 + 2는?\",\n",
    "            \"warning\": \"프로젝트 분석으로 답변하면 과적합\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🔬 고급 과적합 테스트 시작\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    overfitting_count = 0\n",
    "    \n",
    "    for i, test in enumerate(challenging_tests, 1):\n",
    "        print(f\"\\n{i}. {test['name']}\")\n",
    "        print(f\"📝 질문: {test['prompt']}\")\n",
    "        print(f\"⚠️ 과적합 징후: {test['warning']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"당신은 도움이 되는 AI 어시스턴트입니다.\"},\n",
    "                    {\"role\": \"user\", \"content\": test['prompt']}\n",
    "                ],\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            print(f\"🤖 응답: {result}\")\n",
    "            \n",
    "            # 과적합 징후 체크\n",
    "            overfitting_signs = []\n",
    "            \n",
    "            if any(keyword in result.lower() for keyword in [\n",
    "                \"project_summary\", \"erd\", \"api\", \"데이터베이스\", \"테이블\", \n",
    "                \"relationship\", \"스키마\", \"엔드포인트\"\n",
    "            ]):\n",
    "                overfitting_signs.append(\"기술적 용어 사용\")\n",
    "            \n",
    "            if \"{\" in result and \"}\" in result:\n",
    "                overfitting_signs.append(\"JSON 구조 강요\")\n",
    "            \n",
    "            if len(result) > 1000:\n",
    "                overfitting_signs.append(\"과도하게 긴 응답\")\n",
    "            \n",
    "            if \"프로젝트\" in result and test['name'] in [\"완전히 다른 도메인\", \"창의적 요청\", \"개인적 조언\"]:\n",
    "                overfitting_signs.append(\"부적절한 프로젝트 언급\")\n",
    "            \n",
    "            if overfitting_signs:\n",
    "                print(f\"🚨 과적합 징후 발견: {', '.join(overfitting_signs)}\")\n",
    "                overfitting_count += 1\n",
    "            else:\n",
    "                print(\"✅ 정상적인 응답\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테스트 실패: {e}\")\n",
    "    \n",
    "    # 결과 평가\n",
    "    overfitting_rate = (overfitting_count / len(challenging_tests)) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"🎯 고급 테스트 결과\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"📊 과적합 징후: {overfitting_count}/{len(challenging_tests)} ({overfitting_rate:.1f}%)\")\n",
    "    \n",
    "    if overfitting_rate == 0:\n",
    "        print(\"🎉 훌륭함! 과적합 없이 잘 훈련됨\")\n",
    "        print(\"✅ 모델 사용 가능\")\n",
    "    elif overfitting_rate <= 25:\n",
    "        print(\"👍 양호함. 경미한 과적합 있지만 사용 가능\")\n",
    "    elif overfitting_rate <= 50:\n",
    "        print(\"⚠️ 주의 필요. 일부 과적합 존재\")\n",
    "    else:\n",
    "        print(\"🚨 심각한 과적합. 재훈련 권장\")\n",
    "    \n",
    "    return overfitting_rate\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"원본 GPT-4o-mini와 비교\"\"\"\n",
    "    print(\"🔄 원본 모델과 비교 테스트\")\n",
    "    print(\"같은 질문을 원본 gpt-4o-mini에도 물어보세요:\")\n",
    "    \n",
    "    test_prompt = \"간단한 웹사이트 만들어줘\"\n",
    "    \n",
    "    print(f\"📝 테스트 질문: {test_prompt}\")\n",
    "    print(\"\\n💡 비교 포인트:\")\n",
    "    print(\"1. 응답 길이 차이\")\n",
    "    print(\"2. 전문 용어 사용 빈도\")\n",
    "    print(\"3. 구조화 정도\")\n",
    "    print(\"4. 실용성\")\n",
    "    \n",
    "    # 원본 모델로 테스트 (참고용)\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        original_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 도움이 되는 AI 어시스턴트입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🤖 원본 gpt-4o-mini 응답:\")\n",
    "        print(f\"{original_response.choices[0].message.content[:200]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"원본 모델 테스트 실패: {e}\")\n",
    "\n",
    "def final_recommendation():\n",
    "    \"\"\"최종 권장사항\"\"\"\n",
    "    print(\"\\n🎯 최종 평가 및 권장사항\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    choice = input(\"\"\"\n",
    "현재 모델 상태를 어떻게 평가하시겠습니까?\n",
    "\n",
    "1. 😊 만족 - 현재 모델 사용\n",
    "2. 🤔 확실하지 않음 - 더 많은 테스트 필요  \n",
    "3. 😟 불만족 - 재훈련 필요\n",
    "\n",
    "선택 (1/2/3): \"\"\").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        print(\"\\n✅ 훌륭합니다! 현재 모델 사용을 권장합니다.\")\n",
    "        print(\"💡 다음 번 파인튜닝에서는:\")\n",
    "        print(\"  - 더 다양한 도메인 데이터 추가\")\n",
    "        print(\"  - validation 데이터 포함 고려\")\n",
    "        \n",
    "    elif choice == \"2\":\n",
    "        print(\"\\n🔍 추가 테스트를 권장합니다:\")\n",
    "        print(\"  - 실제 사용 사례로 1주일 테스트\")\n",
    "        print(\"  - 다양한 사용자에게 피드백 요청\")\n",
    "        print(\"  - A/B 테스트 (원본 vs 파인튜닝 모델)\")\n",
    "        \n",
    "    elif choice == \"3\":\n",
    "        print(\"\\n🔧 재훈련 권장 설정:\")\n",
    "        print(\"  - n_epochs: 1\")\n",
    "        print(\"  - learning_rate_multiplier: 0.1\")\n",
    "        print(\"  - 더 다양한 훈련 데이터 추가\")\n",
    "        print(\"  - validation 데이터 포함\")\n",
    "    \n",
    "    else:\n",
    "        print(\"올바른 선택지를 입력해주세요.\")\n",
    "\n",
    "# 실행 함수\n",
    "if __name__ == \"__main__\":\n",
    "    model_id = input(\"테스트할 모델 ID를 입력하세요: \").strip()\n",
    "    \n",
    "    if model_id.startswith(\"ft:\"):\n",
    "        print(\"🚀 고급 테스트 시작...\")\n",
    "        overfitting_rate = advanced_overfitting_test(model_id)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        compare_with_original()\n",
    "        final_recommendation()\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 올바른 파인튜닝 모델 ID를 입력해주세요 (ft:로 시작)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0e9cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 프로젝트 분석 AI 파인튜닝 성과 테스트 시작!\n",
      "🎯 프로젝트 분석 AI 성능 테스트\n",
      "============================================================\n",
      "\n",
      "1. 새로운 프로젝트 - 도서관 시스템\n",
      "📝 입력: 도서관 관리 시스템을 개발하려고 합니다. \n",
      "도서 대출/반납, 회원 관리, 도서 검색 기능이 필요하고, \n",
      "사서와 일반 회원의 권한을 구분해야 합니다. \n",
      "웹 기반으로 만들 예정입니다.\n",
      "⏳ 분석 중...\n",
      "✅ 응답 완료 (32.5초)\n",
      "📊 응답 길이: 9475자\n",
      "🎯 품질 점수: 8.100000000000001/10\n",
      "\n",
      "📄 응답 미리보기:\n",
      "----------------------------------------\n",
      "**프로젝트 상세 정보:**\n",
      "{'project_summary': {'title': '도서관 관리 시스템', 'category': '웹 애플리케이션', 'target_users': ['사서', '일반 회원'], 'main_purpose': '효율적인 도서 대출/반납 및 회원 관리', 'key_features': [{'feature': '도서 대출/반납', 'description': '회원이 도서를 대출하고 반납할 수 있는 기능'}, {'feature': '회원 관리', 'description': '회원 정보를 등록, 수정, 삭제할 수...\n",
      "----------------------------------------\n",
      "\n",
      "2. 간단한 아이디어 - 카페 주문 앱\n",
      "📝 입력: 카페에서 사용할 주문 앱을 만들고 싶습니다. 메뉴 보기, 주문하기, 결제 기능이 필요해요.\n",
      "⏳ 분석 중...\n",
      "✅ 응답 완료 (35.2초)\n",
      "📊 응답 길이: 8766자\n",
      "🎯 품질 점수: 8.100000000000001/10\n",
      "\n",
      "📄 응답 미리보기:\n",
      "----------------------------------------\n",
      "프로젝트 상세 정보를 기반으로 카페 주문 앱을 위한 개발 계획을 제시합니다.\n",
      "\n",
      "### 프로젝트 상세 정보\n",
      "{'project_summary': {'title': '카페 주문 앱', 'category': '모바일 애플리케이션', 'target_users': ['카페 고객', '카페 직원'], 'main_purpose': '카페에서의 주문 및 결제를 간편하게 처리하기 위한 모바일 애플리케이션', 'key_features': [{'feature': '메뉴 보기', 'description': '카페의 전체 메뉴를 확인하고, 각 메뉴의 상세 ...\n",
      "----------------------------------------\n",
      "\n",
      "3. 복잡한 프로젝트 - 온라인 교육\n",
      "📝 입력: 온라인 교육 플랫폼을 개발하려고 합니다.\n",
      "강사가 강의를 업로드하고, 학생이 수강할 수 있으며,\n",
      "과제 제출, 성적 관리, 화상 수업 기능까지 포함해야 합니다.\n",
      "사용자는 학생, 강사, 관리자로 구분됩니다.\n",
      "⏳ 분석 중...\n",
      "✅ 응답 완료 (36.8초)\n",
      "📊 응답 길이: 9754자\n",
      "🎯 품질 점수: 8.5/10\n",
      "\n",
      "📄 응답 미리보기:\n",
      "----------------------------------------\n",
      "**프로젝트 상세 정보:**\n",
      "{'project_summary': {'title': '온라인 교육 플랫폼', 'category': '웹 애플리케이션', 'target_users': ['학생', '강사', '관리자'], 'main_purpose': '효율적인 온라인 교육 제공 및 학습 관리', 'key_features': [{'feature': '강의 업로드', 'description': '강사가 강의를 플랫폼에 업로드할 수 있는 기능'}, {'feature': '수강 기능', 'description': '학생이 강의를 수강할 수 있는...\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "🎯 전체 테스트 결과 분석\n",
      "============================================================\n",
      "📊 성공한 테스트: 3/3\n",
      "⏱️ 평균 응답 시간: 34.8초\n",
      "🎯 평균 품질 점수: 8.2/10\n",
      "📝 평균 응답 길이: 9332자\n",
      "\n",
      "📋 상세 품질 분석:\n",
      "  structure: 1.3/2.5\n",
      "  technical_accuracy: 2.4/2.5\n",
      "  completeness: 2.0/2.5\n",
      "  practicality: 2.5/2.5\n",
      "\n",
      "🎉 훌륭함! 프로젝트 분석 AI가 성공적으로 파인튜닝됨\n",
      "✅ 실무에서 바로 사용 가능한 수준\n",
      "\n",
      "🔄 원본 GPT-4o-mini와 성능 비교\n",
      "==================================================\n",
      "🤖 파인튜닝 모델 응답:\n",
      "------------------------------\n",
      "간단한 블로그 플랫폼을 구축하기 위한 기본적인 요구사항과 기능을 정리해보겠습니다.\n",
      "\n",
      "### 프로젝트 명\n",
      "간단한 블로그 플랫폼\n",
      "\n",
      "### 주요 기능\n",
      "1. **사용자 관리**\n",
      "   - 사용자 가입 및 로그인 기능\n",
      "   - 사용자 프로필 관리\n",
      "\n",
      "2. **글 작성 및 관리**\n",
      "   - 글 작성, 수정, 삭제 기능\n",
      "   - 글 제목, 내용, 태그 입력 가능\n",
      "   - 카테고리별로 글 분류 (예: 기술, 여행, 음식 등)\n",
      "\n",
      "3. **댓글 기능**\n",
      "   - 글에 댓글 작성, 수정, 삭제 기능\n",
      "   - 댓글 작성 시 사용자 정보 표시 (닉네임 등)\n",
      "\n",
      "4. **카테고리 관리**\n",
      "   - 카테고리 추가, 수정, 삭제 기능\n",
      "   - 각 글에 해당 카테고리 지정 가능\n",
      "\n",
      "5. **검색 및 필터링**\n",
      "   - 글 제목 및 내용으로 검색...\n",
      "\n",
      "🔸 원본 gpt-4o-mini 응답:\n",
      "------------------------------\n",
      "블로그 플랫폼 개발을 위한 기본 구조와 기능을 설계해보겠습니다. 이 플랫폼은 사용자 인증, 글 작성, 댓글 달기, 그리고 카테고리 분류 기능을 포함합니다. \n",
      "\n",
      "### 1. 요구 사항 정의\n",
      "\n",
      "#### 기본 기능\n",
      "1. **사용자 관리**\n",
      "   - 사용자 가입 및 로그인 기능\n",
      "   - 비밀번호 재설정 기능\n",
      "   - 사용자 프로필 관리\n",
      "\n",
      "2. **블로그 글 관리**\n",
      "   - 글 작성, 수정, 삭제 기능\n",
      "   - 각 글에는 제목, 내용, 작성일, 작성자 정보 포함\n",
      "   - 카테고리 선택 기능 (여러 개 선택 가능)\n",
      "\n",
      "3. **댓글 기능**\n",
      "   - 각 글에 댓글 작성, 수정, 삭제 가능\n",
      "   - 댓글 작성자는 익명 또는 사용자 계정으로 표시\n",
      "\n",
      "4. **카테고리 관리**\n",
      "   - 카테고리 추가, 수정, 삭제\n",
      "   - ...\n",
      "\n",
      "📊 비교 분석:\n",
      "  파인튜닝 길이: 922자\n",
      "  원본 길이: 1537자\n",
      "  파인튜닝 구조화: ❌\n",
      "  원본 구조화: ✅\n",
      "⚠️ 파인튜닝 효과 미흡. 추가 개선 필요\n",
      "\n",
      "🎯 최종 결론:\n",
      "이 테스트 결과를 바탕으로 파인튜닝의 성공 여부를 판단할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "파인튜닝된 프로젝트 분석 AI 성능 테스트\n",
    "원래 목표: 프로젝트 아이디어 → 체계적 분석 + ERD + API 설계\n",
    "\"\"\"\n",
    "\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "\n",
    "def test_project_analysis_ai():\n",
    "    \"\"\"프로젝트 분석 AI 핵심 기능 테스트\"\"\"\n",
    "    \n",
    "    model_id = \"ft:gpt-4o-mini-2024-07-18:test::BebIPMSD\"\n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    # 원래 목표에 맞는 테스트 케이스들\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"새로운 프로젝트 - 도서관 시스템\",\n",
    "            \"prompt\": \"\"\"도서관 관리 시스템을 개발하려고 합니다. \n",
    "도서 대출/반납, 회원 관리, 도서 검색 기능이 필요하고, \n",
    "사서와 일반 회원의 권한을 구분해야 합니다. \n",
    "웹 기반으로 만들 예정입니다.\"\"\",\n",
    "            \"expected_elements\": [\n",
    "                \"프로젝트 상세 정보\",\n",
    "                \"ERD 데이터\", \n",
    "                \"API 명세\",\n",
    "                \"관계 데이터\",\n",
    "                \"테이블 설계\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"간단한 아이디어 - 카페 주문 앱\",\n",
    "            \"prompt\": \"카페에서 사용할 주문 앱을 만들고 싶습니다. 메뉴 보기, 주문하기, 결제 기능이 필요해요.\",\n",
    "            \"expected_elements\": [\n",
    "                \"프로젝트 분석\",\n",
    "                \"데이터베이스 설계\", \n",
    "                \"API 설계\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"복잡한 프로젝트 - 온라인 교육\",\n",
    "            \"prompt\": \"\"\"온라인 교육 플랫폼을 개발하려고 합니다.\n",
    "강사가 강의를 업로드하고, 학생이 수강할 수 있으며,\n",
    "과제 제출, 성적 관리, 화상 수업 기능까지 포함해야 합니다.\n",
    "사용자는 학생, 강사, 관리자로 구분됩니다.\"\"\",\n",
    "            \"expected_elements\": [\n",
    "                \"복잡한 ERD\",\n",
    "                \"다양한 API 엔드포인트\",\n",
    "                \"권한 관리\",\n",
    "                \"파일 업로드\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🎯 프로젝트 분석 AI 성능 테스트\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\n{i}. {test['name']}\")\n",
    "        print(f\"📝 입력: {test['prompt']}\")\n",
    "        print(\"⏳ 분석 중...\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\", \n",
    "                        \"content\": \"\"\"당신은 프로젝트 아이디어를 체계적으로 분석하고 구조화하여 구체적인 개발 계획을 제시하는 전문 AI 어시스턴트입니다.\n",
    "\n",
    "## 주요 역할과 능력:\n",
    "\n",
    "### 1. 프로젝트 분석 전문가\n",
    "- 사용자가 제공하는 프로젝트 아이디어나 설명을 깊이 있게 분석합니다\n",
    "- 핵심 기능, 대상 사용자, 기술 스택, 비즈니스 모델 등을 체계적으로 파악합니다\n",
    "- 프로젝트의 문제 해결 방향과 기대 효과를 명확히 정의합니다\n",
    "\n",
    "### 2. 데이터베이스 설계 전문가\n",
    "- 프로젝트 요구사항을 바탕으로 최적화된 ERD(Entity Relationship Diagram)를 설계합니다\n",
    "- 테이블 간의 관계, 외래키 제약조건, 데이터 타입을 정확히 정의합니다\n",
    "- 확장성과 성능을 고려한 데이터베이스 구조를 제안합니다\n",
    "\n",
    "### 3. API 설계 전문가\n",
    "- RESTful API 원칙에 따라 체계적인 API 명세를 작성합니다\n",
    "- OpenAPI(Swagger) 3.0 표준을 준수하여 완전한 API 문서를 생성합니다\n",
    "- 각 엔드포인트별 요청/응답 스키마, 에러 처리, 인증 방식을 상세히 정의합니다\n",
    "\n",
    "항상 체계적이고 전문적인 관점에서 프로젝트를 분석하며, 개발팀이 바로 실행에 옮길 수 있는 구체적인 가이드를 제공하는 것이 목표입니다.\"\"\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": test['prompt']}\n",
    "                ],\n",
    "                max_tokens=3000,  # 충분한 토큰 할당\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            print(f\"✅ 응답 완료 ({response_time:.1f}초)\")\n",
    "            print(f\"📊 응답 길이: {len(result)}자\")\n",
    "            \n",
    "            # 응답 품질 분석\n",
    "            quality_score = analyze_response_quality(result, test['expected_elements'])\n",
    "            \n",
    "            print(f\"🎯 품질 점수: {quality_score['total_score']}/10\")\n",
    "            \n",
    "            # 결과 저장\n",
    "            results.append({\n",
    "                'test_name': test['name'],\n",
    "                'response_time': response_time,\n",
    "                'response_length': len(result),\n",
    "                'quality_score': quality_score,\n",
    "                'full_response': result[:500] + \"...\" if len(result) > 500 else result\n",
    "            })\n",
    "            \n",
    "            # 응답 일부 출력\n",
    "            print(f\"\\n📄 응답 미리보기:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(result[:300] + \"...\" if len(result) > 300 else result)\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테스트 실패: {e}\")\n",
    "            results.append({\n",
    "                'test_name': test['name'],\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # 전체 결과 분석\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🎯 전체 테스트 결과 분석\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    successful_tests = [r for r in results if 'error' not in r]\n",
    "    \n",
    "    if successful_tests:\n",
    "        avg_response_time = sum(r['response_time'] for r in successful_tests) / len(successful_tests)\n",
    "        avg_quality = sum(r['quality_score']['total_score'] for r in successful_tests) / len(successful_tests)\n",
    "        avg_length = sum(r['response_length'] for r in successful_tests) / len(successful_tests)\n",
    "        \n",
    "        print(f\"📊 성공한 테스트: {len(successful_tests)}/{len(results)}\")\n",
    "        print(f\"⏱️ 평균 응답 시간: {avg_response_time:.1f}초\")\n",
    "        print(f\"🎯 평균 품질 점수: {avg_quality:.1f}/10\")\n",
    "        print(f\"📝 평균 응답 길이: {avg_length:.0f}자\")\n",
    "        \n",
    "        # 상세 분석\n",
    "        print(f\"\\n📋 상세 품질 분석:\")\n",
    "        quality_categories = ['structure', 'technical_accuracy', 'completeness', 'practicality']\n",
    "        \n",
    "        for category in quality_categories:\n",
    "            category_scores = [r['quality_score'][category] for r in successful_tests]\n",
    "            avg_category = sum(category_scores) / len(category_scores)\n",
    "            print(f\"  {category}: {avg_category:.1f}/2.5\")\n",
    "        \n",
    "        # 전체 평가\n",
    "        if avg_quality >= 8:\n",
    "            print(f\"\\n🎉 훌륭함! 프로젝트 분석 AI가 성공적으로 파인튜닝됨\")\n",
    "            print(f\"✅ 실무에서 바로 사용 가능한 수준\")\n",
    "        elif avg_quality >= 6:\n",
    "            print(f\"\\n👍 양호함! 기본적인 분석 기능 잘 작동\")\n",
    "            print(f\"💡 일부 개선 여지 있음\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️ 개선 필요! 추가 훈련 또는 프롬프트 조정 권장\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_response_quality(response: str, expected_elements: list) -> dict:\n",
    "    \"\"\"응답 품질 분석\"\"\"\n",
    "    \n",
    "    score = {\n",
    "        'structure': 0,      # 구조화 정도 (0-2.5)\n",
    "        'technical_accuracy': 0,  # 기술적 정확성 (0-2.5)\n",
    "        'completeness': 0,   # 완성도 (0-2.5)\n",
    "        'practicality': 0,   # 실용성 (0-2.5)\n",
    "        'total_score': 0\n",
    "    }\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    # 1. 구조화 정도 체크\n",
    "    structure_keywords = ['프로젝트', 'erd', 'api', '데이터베이스', '테이블', '관계']\n",
    "    found_structure = sum(1 for keyword in structure_keywords if keyword in response_lower)\n",
    "    score['structure'] = min(found_structure * 0.4, 2.5)\n",
    "    \n",
    "    # 2. 기술적 정확성\n",
    "    technical_keywords = ['primary_key', 'foreign_key', 'varchar', 'integer', 'post', 'get']\n",
    "    found_technical = sum(1 for keyword in technical_keywords if keyword in response_lower)\n",
    "    score['technical_accuracy'] = min(found_technical * 0.4, 2.5)\n",
    "    \n",
    "    # 3. 완성도 (JSON 구조 등)\n",
    "    if '{' in response and '}' in response:\n",
    "        score['completeness'] += 1.0\n",
    "    if 'project_summary' in response_lower or '프로젝트 상세' in response:\n",
    "        score['completeness'] += 0.5\n",
    "    if 'erd_data' in response_lower or 'erd 데이터' in response:\n",
    "        score['completeness'] += 0.5\n",
    "    if 'api' in response_lower:\n",
    "        score['completeness'] += 0.5\n",
    "    \n",
    "    # 4. 실용성\n",
    "    if len(response) > 1000:  # 충분히 상세한 응답\n",
    "        score['practicality'] += 1.0\n",
    "    if 'openapi' in response_lower or 'swagger' in response_lower:\n",
    "        score['practicality'] += 0.5\n",
    "    if any(word in response_lower for word in ['실무', '개발팀', '구현', '배포']):\n",
    "        score['practicality'] += 1.0\n",
    "    \n",
    "    score['total_score'] = sum(score[key] for key in score if key != 'total_score')\n",
    "    \n",
    "    return score\n",
    "\n",
    "def compare_with_original_gpt():\n",
    "    \"\"\"원본 GPT-4o-mini와 비교\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔄 원본 GPT-4o-mini와 성능 비교\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_prompt = \"\"\"간단한 블로그 플랫폼을 만들고 싶습니다. \n",
    "사용자가 글을 작성하고, 댓글을 달 수 있으며, \n",
    "카테고리별로 글을 분류할 수 있는 기능이 필요합니다.\"\"\"\n",
    "    \n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    # 파인튜닝 모델 테스트\n",
    "    print(\"🤖 파인튜닝 모델 응답:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        finetuned_response = client.chat.completions.create(\n",
    "            model=\"ft:gpt-4o-mini-2024-07-18:test::BebIPMSD\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 프로젝트 분석 전문가입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        ft_result = finetuned_response.choices[0].message.content\n",
    "        print(ft_result[:400] + \"...\" if len(ft_result) > 400 else ft_result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"파인튜닝 모델 테스트 실패: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 원본 모델 테스트\n",
    "    print(f\"\\n🔸 원본 gpt-4o-mini 응답:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        original_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 프로젝트 분석 전문가입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        orig_result = original_response.choices[0].message.content\n",
    "        print(orig_result[:400] + \"...\" if len(orig_result) > 400 else orig_result)\n",
    "        \n",
    "        # 비교 분석\n",
    "        print(f\"\\n📊 비교 분석:\")\n",
    "        print(f\"  파인튜닝 길이: {len(ft_result)}자\")\n",
    "        print(f\"  원본 길이: {len(orig_result)}자\")\n",
    "        \n",
    "        ft_has_structure = any(word in ft_result.lower() for word in ['erd', 'api', '데이터베이스', 'project_summary'])\n",
    "        orig_has_structure = any(word in orig_result.lower() for word in ['erd', 'api', '데이터베이스', 'project_summary'])\n",
    "        \n",
    "        print(f\"  파인튜닝 구조화: {'✅' if ft_has_structure else '❌'}\")\n",
    "        print(f\"  원본 구조화: {'✅' if orig_has_structure else '❌'}\")\n",
    "        \n",
    "        if ft_has_structure and not orig_has_structure:\n",
    "            print(\"🎉 파인튜닝 효과 확인! 더 구조화된 응답 제공\")\n",
    "        elif not ft_has_structure:\n",
    "            print(\"⚠️ 파인튜닝 효과 미흡. 추가 개선 필요\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"원본 모델 테스트 실패: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 프로젝트 분석 AI 파인튜닝 성과 테스트 시작!\")\n",
    "    \n",
    "    # 메인 테스트 실행\n",
    "    results = test_project_analysis_ai()\n",
    "    \n",
    "    # 원본과 비교\n",
    "    compare_with_original_gpt()\n",
    "    \n",
    "    print(f\"\\n🎯 최종 결론:\")\n",
    "    print(\"이 테스트 결과를 바탕으로 파인튜닝의 성공 여부를 판단할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec73f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 파인튜닝 모델 최적화 가이드\n",
      "🧪 일관성 테스트 (같은 질문 3번)\n",
      "==================================================\n",
      "\n",
      "테스트 1:\n",
      "  길이: 11243자\n",
      "  품질: 10/10\n",
      "  구조화: ✅\n",
      "  ERD 포함: ✅\n",
      "  API 포함: ✅\n",
      "\n",
      "테스트 2:\n",
      "  길이: 11706자\n",
      "  품질: 10/10\n",
      "  구조화: ✅\n",
      "  ERD 포함: ✅\n",
      "  API 포함: ✅\n",
      "\n",
      "테스트 3:\n",
      "  길이: 11578자\n",
      "  품질: 10/10\n",
      "  구조화: ✅\n",
      "  ERD 포함: ✅\n",
      "  API 포함: ✅\n",
      "\n",
      "📊 일관성 분석:\n",
      "  평균 점수: 10.0/10\n",
      "  점수 편차: 0\n",
      "✅ 일관된 성능\n",
      "\n",
      "📋 파인튜닝 모델 최적 사용 가이드\n",
      "==================================================\n",
      "\n",
      "### ✅ 최고 성능을 위한 사용법\n",
      "\n",
      "1. **전체 시스템 프롬프트 사용**\n",
      "   - 항상 완전한 시스템 프롬프트를 포함하세요\n",
      "   - 역할과 목표를 명확히 명시하세요\n",
      "\n",
      "2. **구조화된 요청**\n",
      "   - \"다음 형식으로 응답해주세요\" 추가\n",
      "   - 원하는 섹션을 명시하세요\n",
      "\n",
      "3. **충분한 토큰 할당**\n",
      "   - max_tokens: 3000-4000 권장\n",
      "   - 복잡한 프로젝트는 더 높게 설정\n",
      "\n",
      "4. **일관성을 위한 설정**\n",
      "   - temperature: 0.3 (창의성 < 일관성)\n",
      "   - 같은 시스템 프롬프트 재사용\n",
      "\n",
      "### ❌ 피해야 할 사용법\n",
      "\n",
      "1. 간단한 시스템 프롬프트 사용\n",
      "2. max_tokens 제한 (1000 이하)\n",
      "3. 높은 temperature 설정 (0.8+)\n",
      "4. 구조화 요청 없이 모호한 질문\n",
      "\n",
      "### 🎯 예상 성능\n",
      "\n",
      "- **완벽한 설정**: 8-10/10 품질\n",
      "- **기본 설정**: 6-8/10 품질  \n",
      "- **부적절한 설정**: 4-6/10 품질\n",
      "\n",
      "\n",
      "🎯 최종 권장사항\n",
      "========================================\n",
      "\n",
      "## 🎉 파인튜닝 성공! \n",
      "\n",
      "### 현재 상태\n",
      "- ✅ 핵심 기능 완벽 작동 (8.2/10)\n",
      "- ✅ 실무 사용 가능 수준\n",
      "- ✅ 복잡한 프로젝트 분석 능력 우수\n",
      "- ⚠️ 일관성 약간 부족 (프롬프트 의존적)\n",
      "\n",
      "### 사용 권장사항\n",
      "\n",
      "1. **즉시 사용 가능**: 현재 모델로 충분히 실용적\n",
      "2. **최적 프롬프트 사용**: 제공된 시스템 프롬프트 필수\n",
      "3. **설정 최적화**: temperature 0.3, max_tokens 3000+\n",
      "\n",
      "### 추가 개선 방향 (선택사항)\n",
      "\n",
      "만약 더 나은 일관성을 원한다면:\n",
      "- validation 데이터 추가하여 재훈련\n",
      "- 더 다양한 도메인 데이터 포함\n",
      "- epoch 수 조정 (현재 3 → 2)\n",
      "\n",
      "하지만 **현재 상태로도 충분히 우수합니다!**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "파인튜닝된 모델의 성능을 최대화하는 사용 가이드\n",
    "일관된 고품질 응답을 위한 최적화된 프롬프트\n",
    "\"\"\"\n",
    "\n",
    "import openai\n",
    "\n",
    "# 최적화된 시스템 프롬프트\n",
    "OPTIMIZED_SYSTEM_PROMPT = \"\"\"당신은 프로젝트 아이디어를 체계적으로 분석하고 구조화하여 구체적인 개발 계획을 제시하는 전문 AI 어시스턴트입니다.\n",
    "\n",
    "## 주요 역할과 능력:\n",
    "\n",
    "### 1. 프로젝트 분석 전문가\n",
    "- 사용자가 제공하는 프로젝트 아이디어나 설명을 깊이 있게 분석합니다\n",
    "- 핵심 기능, 대상 사용자, 기술 스택, 비즈니스 모델 등을 체계적으로 파악합니다\n",
    "- 프로젝트의 문제 해결 방향과 기대 효과를 명확히 정의합니다\n",
    "\n",
    "### 2. 데이터베이스 설계 전문가\n",
    "- 프로젝트 요구사항을 바탕으로 최적화된 ERD(Entity Relationship Diagram)를 설계합니다\n",
    "- 테이블 간의 관계, 외래키 제약조건, 데이터 타입을 정확히 정의합니다\n",
    "- 확장성과 성능을 고려한 데이터베이스 구조를 제안합니다\n",
    "\n",
    "### 3. API 설계 전문가\n",
    "- RESTful API 원칙에 따라 체계적인 API 명세를 작성합니다\n",
    "- OpenAPI(Swagger) 3.0 표준을 준수하여 완전한 API 문서를 생성합니다\n",
    "- 각 엔드포인트별 요청/응답 스키마, 에러 처리, 인증 방식을 상세히 정의합니다\n",
    "\n",
    "## 응답 형식:\n",
    "모든 응답은 다음과 같은 구조화된 형태로 제공해야 합니다:\n",
    "\n",
    "1. **프로젝트 상세 정보**: 제목, 카테고리, 대상 사용자, 핵심 기능, 기술 스택, 문제 해결 방안 등을 포함한 종합 분석\n",
    "2. **관계 데이터**: 데이터베이스 테이블 간의 관계와 외래키 제약조건 정의\n",
    "3. **ERD 데이터**: 각 테이블의 속성, 데이터 타입, 키 정보를 포함한 완전한 스키마\n",
    "4. **API 명세 데이터**: OpenAPI 3.0 표준을 준수한 완전한 API 문서\n",
    "\n",
    "항상 체계적이고 전문적인 관점에서 프로젝트를 분석하며, 개발팀이 바로 실행에 옮길 수 있는 구체적인 가이드를 제공하는 것이 목표입니다.\"\"\"\n",
    "\n",
    "class OptimizedProjectAnalysisAI:\n",
    "    \"\"\"최적화된 프로젝트 분석 AI 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, model_id: str = \"ft:gpt-4o-mini-2024-07-18:test::BebIPMSD\"):\n",
    "        self.client = openai.OpenAI()\n",
    "        self.model_id = model_id\n",
    "        self.system_prompt = OPTIMIZED_SYSTEM_PROMPT\n",
    "    \n",
    "    def analyze_project(self, project_description: str, force_structured: bool = True) -> str:\n",
    "        \"\"\"프로젝트 분석 수행\"\"\"\n",
    "        \n",
    "        # 구조화된 응답을 강제하는 프롬프트 추가\n",
    "        if force_structured:\n",
    "            enhanced_prompt = f\"\"\"프로젝트 아이디어를 분석해주세요:\n",
    "\n",
    "{project_description}\n",
    "\n",
    "다음 형식으로 응답해주세요:\n",
    "1. **프로젝트 상세 정보**\n",
    "2. **관계 데이터**  \n",
    "3. **ERD 데이터**\n",
    "4. **API 명세 데이터**\n",
    "\n",
    "체계적이고 완전한 분석을 제공해주세요.\"\"\"\n",
    "        else:\n",
    "            enhanced_prompt = project_description\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": enhanced_prompt}\n",
    "                ],\n",
    "                max_tokens=4000,  # 충분한 토큰 할당\n",
    "                temperature=0.3   # 일관성을 위해 낮은 temperature\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"분석 중 오류 발생: {e}\"\n",
    "    \n",
    "    def quick_analysis(self, project_description: str) -> dict:\n",
    "        \"\"\"빠른 품질 분석\"\"\"\n",
    "        \n",
    "        result = self.analyze_project(project_description)\n",
    "        \n",
    "        # 품질 지표 계산\n",
    "        quality_metrics = {\n",
    "            'response_length': len(result),\n",
    "            'has_project_info': '프로젝트 상세' in result or 'project_summary' in result,\n",
    "            'has_erd': 'ERD' in result or 'erd' in result.lower(),\n",
    "            'has_api': 'API' in result or 'api' in result.lower(),\n",
    "            'has_json_structure': '{' in result and '}' in result,\n",
    "            'structured_format': result.count('**') >= 4,  # 최소 4개의 헤더\n",
    "        }\n",
    "        \n",
    "        # 전체 품질 점수 계산\n",
    "        quality_score = sum([\n",
    "            quality_metrics['has_project_info'] * 2,\n",
    "            quality_metrics['has_erd'] * 2,\n",
    "            quality_metrics['has_api'] * 2,\n",
    "            quality_metrics['has_json_structure'] * 2,\n",
    "            quality_metrics['structured_format'] * 2,\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'result': result,\n",
    "            'quality_metrics': quality_metrics,\n",
    "            'quality_score': quality_score,\n",
    "            'max_score': 10\n",
    "        }\n",
    "\n",
    "def test_consistency():\n",
    "    \"\"\"일관성 테스트\"\"\"\n",
    "    \n",
    "    ai = OptimizedProjectAnalysisAI()\n",
    "    \n",
    "    test_case = \"간단한 블로그 플랫폼을 만들고 싶습니다. 사용자가 글을 작성하고 댓글을 달 수 있는 기능이 필요합니다.\"\n",
    "    \n",
    "    print(\"🧪 일관성 테스트 (같은 질문 3번)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(f\"\\n테스트 {i+1}:\")\n",
    "        analysis = ai.quick_analysis(test_case)\n",
    "        \n",
    "        print(f\"  길이: {analysis['quality_metrics']['response_length']}자\")\n",
    "        print(f\"  품질: {analysis['quality_score']}/10\")\n",
    "        print(f\"  구조화: {'✅' if analysis['quality_metrics']['structured_format'] else '❌'}\")\n",
    "        print(f\"  ERD 포함: {'✅' if analysis['quality_metrics']['has_erd'] else '❌'}\")\n",
    "        print(f\"  API 포함: {'✅' if analysis['quality_metrics']['has_api'] else '❌'}\")\n",
    "        \n",
    "        results.append(analysis['quality_score'])\n",
    "    \n",
    "    # 일관성 분석\n",
    "    avg_score = sum(results) / len(results)\n",
    "    score_variance = max(results) - min(results)\n",
    "    \n",
    "    print(f\"\\n📊 일관성 분석:\")\n",
    "    print(f\"  평균 점수: {avg_score:.1f}/10\")\n",
    "    print(f\"  점수 편차: {score_variance}\")\n",
    "    \n",
    "    if score_variance <= 2:\n",
    "        print(\"✅ 일관된 성능\")\n",
    "    else:\n",
    "        print(\"⚠️ 성능 편차 있음\")\n",
    "\n",
    "def create_usage_guide():\n",
    "    \"\"\"사용 가이드 생성\"\"\"\n",
    "    \n",
    "    print(\"\\n📋 파인튜닝 모델 최적 사용 가이드\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    guide = \"\"\"\n",
    "### ✅ 최고 성능을 위한 사용법\n",
    "\n",
    "1. **전체 시스템 프롬프트 사용**\n",
    "   - 항상 완전한 시스템 프롬프트를 포함하세요\n",
    "   - 역할과 목표를 명확히 명시하세요\n",
    "\n",
    "2. **구조화된 요청**\n",
    "   - \"다음 형식으로 응답해주세요\" 추가\n",
    "   - 원하는 섹션을 명시하세요\n",
    "\n",
    "3. **충분한 토큰 할당**\n",
    "   - max_tokens: 3000-4000 권장\n",
    "   - 복잡한 프로젝트는 더 높게 설정\n",
    "\n",
    "4. **일관성을 위한 설정**\n",
    "   - temperature: 0.3 (창의성 < 일관성)\n",
    "   - 같은 시스템 프롬프트 재사용\n",
    "\n",
    "### ❌ 피해야 할 사용법\n",
    "\n",
    "1. 간단한 시스템 프롬프트 사용\n",
    "2. max_tokens 제한 (1000 이하)\n",
    "3. 높은 temperature 설정 (0.8+)\n",
    "4. 구조화 요청 없이 모호한 질문\n",
    "\n",
    "### 🎯 예상 성능\n",
    "\n",
    "- **완벽한 설정**: 8-10/10 품질\n",
    "- **기본 설정**: 6-8/10 품질  \n",
    "- **부적절한 설정**: 4-6/10 품질\n",
    "\"\"\"\n",
    "    \n",
    "    print(guide)\n",
    "\n",
    "def final_recommendation():\n",
    "    \"\"\"최종 권장사항\"\"\"\n",
    "    \n",
    "    print(\"\\n🎯 최종 권장사항\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    recommendation = \"\"\"\n",
    "## 🎉 파인튜닝 성공! \n",
    "\n",
    "### 현재 상태\n",
    "- ✅ 핵심 기능 완벽 작동 (8.2/10)\n",
    "- ✅ 실무 사용 가능 수준\n",
    "- ✅ 복잡한 프로젝트 분석 능력 우수\n",
    "- ⚠️ 일관성 약간 부족 (프롬프트 의존적)\n",
    "\n",
    "### 사용 권장사항\n",
    "\n",
    "1. **즉시 사용 가능**: 현재 모델로 충분히 실용적\n",
    "2. **최적 프롬프트 사용**: 제공된 시스템 프롬프트 필수\n",
    "3. **설정 최적화**: temperature 0.3, max_tokens 3000+\n",
    "\n",
    "### 추가 개선 방향 (선택사항)\n",
    "\n",
    "만약 더 나은 일관성을 원한다면:\n",
    "- validation 데이터 추가하여 재훈련\n",
    "- 더 다양한 도메인 데이터 포함\n",
    "- epoch 수 조정 (현재 3 → 2)\n",
    "\n",
    "하지만 **현재 상태로도 충분히 우수합니다!**\n",
    "\"\"\"\n",
    "    \n",
    "    print(recommendation)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 파인튜닝 모델 최적화 가이드\")\n",
    "    \n",
    "    # 일관성 테스트\n",
    "    test_consistency()\n",
    "    \n",
    "    # 사용 가이드\n",
    "    create_usage_guide()\n",
    "    \n",
    "    # 최종 권장사항\n",
    "    final_recommendation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b88c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_description' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft:gpt-4o-mini-2024-07-18:test::BebIPMSD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: OPTIMIZED_SYSTEM_PROMPT},  \u001b[38;5;66;03m# 전체 프롬프트 필수\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m다음 형식으로 응답해주세요:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. 프로젝트 상세 정보\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. ERD 데이터\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. API 명세\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      6\u001b[0m     ],\n\u001b[0;32m      7\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m,    \u001b[38;5;66;03m# 충분한 토큰\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m     \u001b[38;5;66;03m# 일관성 우선\u001b[39;00m\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'project_description' is not defined"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-4o-mini-2024-07-18:test::BebIPMSD\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": OPTIMIZED_SYSTEM_PROMPT},  # 전체 프롬프트 필수\n",
    "        {\"role\": \"user\", \"content\": f\"{project_description}\\n\\n다음 형식으로 응답해주세요:\\n1. 프로젝트 상세 정보\\n2. ERD 데이터\\n3. API 명세\"}\n",
    "    ],\n",
    "    max_tokens=4000,    # 충분한 토큰\n",
    "    temperature=0.3     # 일관성 우선\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999fa6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 파인튜닝된 프로젝트 분석 AI 테스트\n",
      "==================================================\n",
      "\n",
      "선택하세요:\n",
      "1. 빠른 테스트 (하나의 프로젝트 분석)\n",
      "2. 배치 테스트 (모든 테스트 케이스)  \n",
      "3. 원본 모델과 비교\n",
      "4. 종료\n",
      "\n",
      "\n",
      "❌ 1-4 중에서 선택해주세요.\n",
      "\n",
      "선택하세요:\n",
      "1. 빠른 테스트 (하나의 프로젝트 분석)\n",
      "2. 배치 테스트 (모든 테스트 케이스)  \n",
      "3. 원본 모델과 비교\n",
      "4. 종료\n",
      "\n",
      "\n",
      "❌ 1-4 중에서 선택해주세요.\n",
      "\n",
      "선택하세요:\n",
      "1. 빠른 테스트 (하나의 프로젝트 분석)\n",
      "2. 배치 테스트 (모든 테스트 케이스)  \n",
      "3. 원본 모델과 비교\n",
      "4. 종료\n",
      "\n",
      "\n",
      "❌ 1-4 중에서 선택해주세요.\n",
      "\n",
      "선택하세요:\n",
      "1. 빠른 테스트 (하나의 프로젝트 분석)\n",
      "2. 배치 테스트 (모든 테스트 케이스)  \n",
      "3. 원본 모델과 비교\n",
      "4. 종료\n",
      "\n",
      "\n",
      "👋 종료합니다!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "파인튜닝된 모델 바로 사용하기\n",
    "복사해서 붙여넣기만 하면 됩니다!\n",
    "\"\"\"\n",
    "\n",
    "import openai\n",
    "\n",
    "# OpenAI 클라이언트 설정\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# 최적화된 시스템 프롬프트\n",
    "OPTIMIZED_SYSTEM_PROMPT = \"\"\"당신은 프로젝트 아이디어를 체계적으로 분석하고 구조화하여 구체적인 개발 계획을 제시하는 전문 AI 어시스턴트입니다.\n",
    "\n",
    "\n",
    "## 주요 역할과 능력:\n",
    "\n",
    "### 1. 프로젝트 분석 전문가\n",
    "- 사용자가 제공하는 프로젝트 아이디어나 설명을 깊이 있게 분석합니다\n",
    "- 핵심 기능, 대상 사용자, 기술 스택, 비즈니스 모델 등을 체계적으로 파악합니다\n",
    "- 프로젝트의 문제 해결 방향과 기대 효과를 명확히 정의합니다\n",
    "\n",
    "### 2. 데이터베이스 설계 전문가\n",
    "- 프로젝트 요구사항을 바탕으로 최적화된 ERD(Entity Relationship Diagram)를 설계합니다\n",
    "- 테이블 간의 관계, 외래키 제약조건, 데이터 타입을 정확히 정의합니다\n",
    "- 확장성과 성능을 고려한 데이터베이스 구조를 제안합니다\n",
    "\n",
    "### 3. API 설계 전문가\n",
    "- RESTful API 원칙에 따라 체계적인 API 명세를 작성합니다\n",
    "- OpenAPI(Swagger) 3.0 표준을 준수하여 완전한 API 문서를 생성합니다\n",
    "- 각 엔드포인트별 요청/응답 스키마, 에러 처리, 인증 방식을 상세히 정의합니다\n",
    "\n",
    "## 응답 형식:\n",
    "모든 응답은 다음과 같은 구조화된 형태로 제공해야 합니다:\n",
    "\n",
    "1. **프로젝트 상세 정보**: 제목, 카테고리, 대상 사용자, 핵심 기능, 기술 스택, 문제 해결 방안 등을 포함한 종합 분석\n",
    "2. **관계 데이터**: 데이터베이스 테이블 간의 관계와 외래키 제약조건 정의\n",
    "3. **ERD 데이터**: 각 테이블의 속성, 데이터 타입, 키 정보를 포함한 완전한 스키마\n",
    "4. **API 명세 데이터**: OpenAPI 3.0 표준을 준수한 완전한 API 문서\n",
    "\n",
    "항상 체계적이고 전문적인 관점에서 프로젝트를 분석하며, 개발팀이 바로 실행에 옮길 수 있는 구체적인 가이드를 제공하는 것이 목표입니다.\"\"\"\n",
    "\n",
    "# 파인튜닝된 모델 ID\n",
    "MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:test::BebIPMSD\"\n",
    "\n",
    "def analyze_project(project_description):\n",
    "    \"\"\"프로젝트 분석 함수\"\"\"\n",
    "    \n",
    "    # 구조화된 요청 프롬프트\n",
    "    enhanced_prompt = f\"\"\"{project_description}\n",
    "\n",
    "다음 형식으로 체계적인 분석을 제공해주세요:\n",
    "\n",
    "1. **프로젝트 상세 정보**\n",
    "2. **ERD 데이터**\n",
    ". **관계 데이터** \n",
    "4. **API 명세 데이터**\n",
    "\n",
    "각각 형식은 다음과 같이 출력이 되고, json 형태에 맞춰서 출력해주세요요\n",
    "\n",
    "**프로젝트 상세 정보**\n",
    "{\n",
    "  \"title\": \"3인칭 카드게임 기반 MMORPG\",\n",
    "  \"category\": \"게임\",\n",
    "  \"target_users\": [\n",
    "    \"게임 애호가\",\n",
    "    \"MMORPG 팬\",\n",
    "    \"카드게임 팬\"\n",
    "  ],\n",
    "  \"core_features\": [\n",
    "    \"3인칭 시점\",\n",
    "    \"랜덤 카드 드롭 시스템\",\n",
    "    \"스토리라인 선택 기능\",\n",
    "    \"가챠 시스템\",\n",
    "    \"즉각적인 판단 요구\"\n",
    "  ],\n",
    "  \"technology_stack\": [\n",
    "    \"Unity 또는 Unreal Engine\",\n",
    "    \"Node.js (서버 사이드)\",\n",
    "    \"MongoDB 또는 MySQL (데이터베이스)\",\n",
    "    \"WebSocket (실시간 통신)\"\n",
    "  ],\n",
    "  \"problem_solving\": {\n",
    "    \"current_problem\": \"기존 카드게임과 MMORPG의 결합 부족\",\n",
    "    \"solution_idea\": \"3인칭 카드게임 요소를 포함한 MMORPG 개발\",\n",
    "    \"expected_benefits\": [\n",
    "      \"사용자의 컨트롤 능력 향상\",\n",
    "      \"다양한 스토리라인 제공\",\n",
    "      \"랜덤성과 전략적 판단의 조화\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "** ERD 데이터**\n",
    "{{\n",
    "  \"erd_table\": [\n",
    "    {{\n",
    "      \"name\": \"테이블명\",\n",
    "      \"erd_column\": [\n",
    "        {{\n",
    "          \"name\": \"컬럼명\",\n",
    "          \"data_type\": \"데이터타입\", \n",
    "          \"is_primary_key\": true/false,\n",
    "          \"is_foreign_key\": true/false,\n",
    "          \"is_nullable\": true/false\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ],\n",
    "  \"erd_relationships\": [\n",
    "    {{\n",
    "      \"from_erd_table_id\": \"시작테이블\",\n",
    "      \"to_erd_table_id\": \"끝테이블\", \n",
    "      \"type\": \"관계타입\",\n",
    "      \"foreign_key\": \"외래키명\",\n",
    "      \"constraint_name\": \"제약조건명\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "반드시 위 json 형식에 맞춰서 진행해주세요\n",
    "\n",
    "실무에서 바로 활용 가능한 구체적이고 완전한 분석을 부탁드립니다.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": OPTIMIZED_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": enhanced_prompt}\n",
    "            ],\n",
    "            max_tokens=4000,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ 오류 발생: {e}\"\n",
    "\n",
    "# 테스트 케이스들\n",
    "TEST_CASES = [\n",
    "    \"음식 배달 앱을 만들고 싶습니다. 고객이 음식을 주문하고, 음식점에서 확인하고, 배달원이 배달하는 시스템이 필요합니다.\",\n",
    "    \n",
    "    \"헬스케어 앱을 개발하려고 합니다. 사용자의 운동 기록, 식단 관리, 건강 데이터 추적 기능이 필요합니다.\",\n",
    "    \n",
    "    \"중고거래 플랫폼을 만들고 싶어요. 물건 등록, 검색, 채팅, 거래 후기 기능이 있었으면 좋겠습니다.\"\n",
    "]\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"빠른 테스트 실행\"\"\"\n",
    "    \n",
    "    print(\"🚀 파인튜닝 모델 빠른 테스트\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 사용자가 직접 입력하거나 테스트 케이스 선택\n",
    "    print(\"테스트 방법을 선택하세요:\")\n",
    "    print(\"1. 직접 프로젝트 아이디어 입력\")\n",
    "    print(\"2. 미리 준비된 테스트 케이스 사용\")\n",
    "    \n",
    "    choice = input(\"선택 (1 또는 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        project_description = input(\"\\n프로젝트 아이디어를 입력하세요: \").strip()\n",
    "        if not project_description:\n",
    "            print(\"❌ 프로젝트 설명을 입력해주세요.\")\n",
    "            return\n",
    "            \n",
    "    elif choice == \"2\":\n",
    "        print(\"\\n준비된 테스트 케이스:\")\n",
    "        for i, case in enumerate(TEST_CASES, 1):\n",
    "            print(f\"{i}. {case[:50]}...\")\n",
    "        \n",
    "        case_choice = input(\"케이스 번호 선택 (1-3): \").strip()\n",
    "        try:\n",
    "            project_description = TEST_CASES[int(case_choice) - 1]\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"❌ 올바른 번호를 선택해주세요.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"❌ 1 또는 2를 선택해주세요.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📝 선택된 프로젝트: {project_description[:100]}...\")\n",
    "    print(\"\\n⏳ 분석 중... (30-40초 소요)\")\n",
    "    \n",
    "    # 분석 실행\n",
    "    result = analyze_project(project_description)\n",
    "    \n",
    "    print(f\"\\n🎯 분석 완료!\")\n",
    "    print(f\"📊 응답 길이: {len(result)}자\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📄 분석 결과:\")\n",
    "    print(\"=\"*60)\n",
    "    print(result)\n",
    "\n",
    "def batch_test():\n",
    "    \"\"\"배치 테스트 - 모든 케이스 테스트\"\"\"\n",
    "    \n",
    "    print(\"🧪 배치 테스트 - 모든 케이스 테스트\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(TEST_CASES, 1):\n",
    "        print(f\"\\n{i}/{len(TEST_CASES)} 테스트 중...\")\n",
    "        print(f\"📝 케이스: {test_case[:80]}...\")\n",
    "        \n",
    "        result = analyze_project(test_case)\n",
    "        \n",
    "        # 품질 체크\n",
    "        quality_indicators = {\n",
    "            'length': len(result),\n",
    "            'has_project_info': '프로젝트 상세' in result or 'project_summary' in result,\n",
    "            'has_erd': 'ERD' in result or 'erd' in result.lower(),\n",
    "            'has_api': 'API' in result or 'api' in result.lower(),\n",
    "            'structured': result.count('**') >= 4\n",
    "        }\n",
    "        \n",
    "        quality_score = sum([\n",
    "            quality_indicators['has_project_info'] * 2,\n",
    "            quality_indicators['has_erd'] * 2, \n",
    "            quality_indicators['has_api'] * 2,\n",
    "            quality_indicators['structured'] * 2,\n",
    "            (quality_indicators['length'] > 5000) * 2\n",
    "        ])\n",
    "        \n",
    "        print(f\"✅ 완료 - 품질: {quality_score}/10, 길이: {quality_indicators['length']}자\")\n",
    "        \n",
    "        results.append({\n",
    "            'case': i,\n",
    "            'quality': quality_score,\n",
    "            'indicators': quality_indicators\n",
    "        })\n",
    "    \n",
    "    # 전체 결과 요약\n",
    "    avg_quality = sum(r['quality'] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"\\n📊 배치 테스트 결과 요약:\")\n",
    "    print(f\"  평균 품질: {avg_quality:.1f}/10\")\n",
    "    print(f\"  성공한 테스트: {len([r for r in results if r['quality'] >= 6])}/{len(results)}\")\n",
    "    \n",
    "    if avg_quality >= 8:\n",
    "        print(\"🎉 우수한 성능!\")\n",
    "    elif avg_quality >= 6:\n",
    "        print(\"👍 양호한 성능!\")\n",
    "    else:\n",
    "        print(\"⚠️ 개선 필요\")\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"원본 모델과 비교\"\"\"\n",
    "    \n",
    "    print(\"🔄 원본 gpt-4o-mini와 비교\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    test_prompt = \"간단한 온라인 서점을 만들고 싶습니다. 책 검색, 장바구니, 주문 기능이 필요합니다.\"\n",
    "    \n",
    "    print(f\"📝 테스트 프롬프트: {test_prompt}\")\n",
    "    \n",
    "    # 파인튜닝 모델\n",
    "    print(f\"\\n🤖 파인튜닝 모델 응답:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    ft_result = analyze_project(test_prompt)\n",
    "    print(f\"길이: {len(ft_result)}자\")\n",
    "    print(f\"미리보기: {ft_result[:200]}...\")\n",
    "    \n",
    "    # 원본 모델  \n",
    "    print(f\"\\n🔸 원본 gpt-4o-mini 응답:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        orig_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 프로젝트 분석 전문가입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        orig_result = orig_response.choices[0].message.content\n",
    "        print(f\"길이: {len(orig_result)}자\")\n",
    "        print(f\"미리보기: {orig_result[:200]}...\")\n",
    "        \n",
    "        # 비교 분석\n",
    "        print(f\"\\n📊 비교 분석:\")\n",
    "        print(f\"  파인튜닝 더 상세함: {'✅' if len(ft_result) > len(orig_result) * 1.5 else '❌'}\")\n",
    "        print(f\"  파인튜닝 구조화: {'✅' if ft_result.count('**') > orig_result.count('**') else '❌'}\")\n",
    "        print(f\"  ERD 포함: {'✅' if 'erd' in ft_result.lower() and 'erd' not in orig_result.lower() else '❌'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"원본 모델 테스트 실패: {e}\")\n",
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 파인튜닝된 프로젝트 분석 AI 테스트\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    menu = \"\"\"\n",
    "선택하세요:\n",
    "1. 빠른 테스트 (하나의 프로젝트 분석)\n",
    "2. 배치 테스트 (모든 테스트 케이스)  \n",
    "3. 원본 모델과 비교\n",
    "4. 종료\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        print(menu)\n",
    "        choice = input(\"선택 (1-4): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            quick_test()\n",
    "        elif choice == \"2\":\n",
    "            batch_test()\n",
    "        elif choice == \"3\":\n",
    "            compare_with_original()\n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 종료합니다!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ 1-4 중에서 선택해주세요.\")\n",
    "        \n",
    "        input(\"\\n계속하려면 엔터를 누르세요...\")\n",
    "\n",
    "# 간단 사용 예시\n",
    "def simple_example():\n",
    "    \"\"\"가장 간단한 사용 예시\"\"\"\n",
    "    \n",
    "    # 프로젝트 설명\n",
    "    project = \"소셜 미디어 앱을 만들고 싶습니다. 게시글 작성, 좋아요, 팔로우 기능이 필요합니다.\"\n",
    "    \n",
    "    # 분석 실행\n",
    "    result = analyze_project(project)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"📄 분석 결과:\")\n",
    "    print(result)\n",
    "\n",
    "# 바로 실행해보려면 이 함수를 호출하세요\n",
    "# simple_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225f3a07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m my_project \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이 프로젝트는 3인칭 카드게임기반 MMORPG 게임 개발 프로젝트입니다. 창업을 목적으로 게임 데이터를 끌어다가 프로젝트를 하려고합니다. 주제는 간단하게 3인칭 카드게임을 mmorpg형식으로 만드려고 합니다. 전체적으로 어떻게 만들생각이냐면 시간이 10초 지날 때마다 하나씩 카드가 드롭되게 하는 형식으로 게임을 만들고싶습니다. 또한, 스토리가 있었으면 좋겠고 선택하는 스토리라인에 따라서 드롭되는 카드의 형식이 달랐으면 좋겠습니다. 이 게임을 통해서 사용자가 랜덤 가챠 + 순간적인 판단으로 컨트롤하는 능력이 늘었으면 좋겠다는 생각으로 프로젝트를 기획하였습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m analyze_project(my_project)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[2], line 93\u001b[0m, in \u001b[0;36manalyze_project\u001b[1;34m(project_description)\u001b[0m\n\u001b[0;32m     49\u001b[0m     enhanced_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_description\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124m다음 형식으로 체계적인 분석을 제공해주세요:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[38;5;124m실무에서 바로 활용 가능한 구체적이고 완전한 분석을 부탁드립니다.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m         response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     94\u001b[0m             model\u001b[38;5;241m=\u001b[39mMODEL_ID,\n\u001b[0;32m     95\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     96\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: OPTIMIZED_SYSTEM_PROMPT},\n\u001b[0;32m     97\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: enhanced_prompt}\n\u001b[0;32m     98\u001b[0m             ],\n\u001b[0;32m     99\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m,\n\u001b[0;32m    100\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m    101\u001b[0m         )\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    922\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    924\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    927\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    928\u001b[0m             {\n\u001b[0;32m    929\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    930\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    949\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    950\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    951\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    952\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    953\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    954\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    955\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    956\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    957\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m    960\u001b[0m             },\n\u001b[0;32m    961\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m    962\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m    963\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m    964\u001b[0m         ),\n\u001b[0;32m    965\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    966\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    967\u001b[0m         ),\n\u001b[0;32m    968\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    969\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    970\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    971\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1238\u001b[0m     )\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:969\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m    967\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    970\u001b[0m         request,\n\u001b[0;32m    971\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    973\u001b[0m     )\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    975\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[0;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Users\\mir96\\anaconda3\\Lib\\ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_project = \"이 프로젝트는 3인칭 카드게임기반 MMORPG 게임 개발 프로젝트입니다. 창업을 목적으로 게임 데이터를 끌어다가 프로젝트를 하려고합니다. 주제는 간단하게 3인칭 카드게임을 mmorpg형식으로 만드려고 합니다. 전체적으로 어떻게 만들생각이냐면 시간이 10초 지날 때마다 하나씩 카드가 드롭되게 하는 형식으로 게임을 만들고싶습니다. 또한, 스토리가 있었으면 좋겠고 선택하는 스토리라인에 따라서 드롭되는 카드의 형식이 달랐으면 좋겠습니다. 이 게임을 통해서 사용자가 랜덤 가챠 + 순간적인 판단으로 컨트롤하는 능력이 늘었으면 좋겠다는 생각으로 프로젝트를 기획하였습니다.\"\n",
    "result = analyze_project(my_project)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d982f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_project(project_description):\n",
    "    \"\"\"프로젝트 분석 함수\"\"\"\n",
    "    \n",
    "    # 구조화된 요청 프롬프트\n",
    "    enhanced_prompt = f\"\"\"{project_description}\n",
    "\n",
    "다음 형식으로 체계적인 분석을 제공해주세요:\n",
    "\n",
    "1. **프로젝트 상세 정보**\n",
    "2. **관계 데이터** \n",
    "3. **ERD 데이터**\n",
    "4. **API 명세 데이터**\n",
    "\n",
    "여기중에서 관계 데이터와 ERD 데이터는 다음과 같은 형식으로 안내해주세요\n",
    "\n",
    "{{\n",
    "  \"erd_table\": [\n",
    "    {{\n",
    "      \"name\": \"테이블명\",\n",
    "      \"erd_column\": [\n",
    "        {{\n",
    "          \"name\": \"컬럼명\",\n",
    "          \"data_type\": \"데이터타입\", \n",
    "          \"is_primary_key\": true/false,\n",
    "          \"is_foreign_key\": true/false,\n",
    "          \"is_nullable\": true/false\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ],\n",
    "  \"erd_relationships\": [\n",
    "    {{\n",
    "      \"from_erd_table_id\": \"시작테이블\",\n",
    "      \"to_erd_table_id\": \"끝테이블\", \n",
    "      \"type\": \"관계타입\",\n",
    "      \"foreign_key\": \"외래키명\",\n",
    "      \"constraint_name\": \"제약조건명\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "반드시 위 json 형식에 맞춰서 진행해주세요\n",
    "\n",
    "실무에서 바로 활용 가능한 구체적이고 완전한 분석을 부탁드립니다.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": OPTIMIZED_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": enhanced_prompt}\n",
    "            ],\n",
    "            max_tokens=4000,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ 오류 발생: {e}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
