"""
GPU ë©”ëª¨ë¦¬ì— ë§ëŠ” QLoRA ì„¤ì • ìë™ ìƒì„±ê¸°
íŒŒì¼ëª…: optimize_gpu_config.py

ì‹¤í–‰ ë°©ë²•:
python optimize_gpu_config.py
"""

import torch
import gc
import psutil
import GPUtil
from peft import LoraConfig
from transformers import BitsAndBytesConfig

def get_gpu_info():
    """GPU ì •ë³´ í™•ì¸"""
    if torch.cuda.is_available():
        gpu = GPUtil.getGPUs()[0]
        total_memory = gpu.memoryTotal
        free_memory = gpu.memoryFree
        used_memory = gpu.memoryUsed
        
        print(f"ğŸ® GPU: {gpu.name}")
        print(f"ğŸ“Š ì´ ë©”ëª¨ë¦¬: {total_memory}MB")
        print(f"ğŸŸ¢ ì‚¬ìš© ê°€ëŠ¥: {free_memory}MB")
        print(f"ğŸ”´ ì‚¬ìš© ì¤‘: {used_memory}MB")
        
        return total_memory, free_memory
    else:
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return 0, 0

def optimize_qlora_config_for_gpu(gpu_memory_mb):
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ QLoRA ì„¤ì • ìµœì í™”"""
    
    configs = {}
    
    if gpu_memory_mb >= 40000:  # 40GB+ (A100, A6000)
        configs = {
            "name": "High-End GPU (40GB+)",
            "max_seq_length": 4096,
            "per_device_batch_size": 4,
            "gradient_accumulation_steps": 4,
            "lora_r": 128,
            "lora_alpha": 256,
            "dataloader_num_workers": 8
        }
    elif gpu_memory_mb >= 24000:  # 24GB (RTX 4090, RTX 3090)
        configs = {
            "name": "High-End Consumer (24GB)",
            "max_seq_length": 2048,
            "per_device_batch_size": 2,
            "gradient_accumulation_steps": 8,
            "lora_r": 64,
            "lora_alpha": 128,
            "dataloader_num_workers": 4
        }
    elif gpu_memory_mb >= 16000:  # 16GB (RTX 4080, RTX 4070 Ti)
        configs = {
            "name": "Mid-Range Consumer (16GB)",
            "max_seq_length": 1024,
            "per_device_batch_size": 1,
            "gradient_accumulation_steps": 16,
            "lora_r": 32,
            "lora_alpha": 64,
            "dataloader_num_workers": 2
        }
    elif gpu_memory_mb >= 12000:  # 12GB (RTX 4070, RTX 3080 Ti)
        configs = {
            "name": "Entry Consumer (12GB)",
            "max_seq_length": 512,
            "per_device_batch_size": 1,
            "gradient_accumulation_steps": 32,
            "lora_r": 16,
            "lora_alpha": 32,
            "dataloader_num_workers": 1
        }
    else:
        configs = {
            "name": "Low Memory (8GB)",
            "max_seq_length": 256,
            "per_device_batch_size": 1,
            "gradient_accumulation_steps": 64,
            "lora_r": 8,
            "lora_alpha": 16,
            "dataloader_num_workers": 1
        }
    
    return configs

def create_memory_efficient_bnb_config(gpu_memory_mb):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ BitsAndBytes ì„¤ì •"""
    
    if gpu_memory_mb < 16000:  # 16GB ë¯¸ë§Œ
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,  # bfloat16 ëŒ€ì‹  float16
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
        )
    else:  # 16GB ì´ìƒ
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
        )

def create_memory_efficient_lora_config(configs):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ LoRA ì„¤ì •"""
    return LoraConfig(
        r=configs["lora_r"],
        lora_alpha=configs["lora_alpha"],
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM",
        inference_mode=False,
    )

def memory_cleanup():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def monitor_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        
        print(f"ğŸ”‹ GPU ë©”ëª¨ë¦¬ - í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB")
    
    # CPU ë©”ëª¨ë¦¬
    cpu_memory = psutil.virtual_memory()
    cpu_used = cpu_memory.used / 1024**3  # GB
    cpu_total = cpu_memory.total / 1024**3  # GB
    
    print(f"ğŸ’¾ CPU ë©”ëª¨ë¦¬ - ì‚¬ìš©: {cpu_used:.2f}GB / {cpu_total:.2f}GB")

def get_recommended_settings():
    """í˜„ì¬ ì‹œìŠ¤í…œì— ë§ëŠ” ê¶Œì¥ ì„¤ì • ì¶œë ¥"""
    total_memory, free_memory = get_gpu_info()
    
    if total_memory == 0:
        print("âš ï¸  GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    configs = optimize_qlora_config_for_gpu(total_memory)
    
    print(f"\nğŸ“‹ {configs['name']} ê¶Œì¥ ì„¤ì •:")
    print(f"â”œâ”€ max_seq_length: {configs['max_seq_length']}")
    print(f"â”œâ”€ per_device_batch_size: {configs['per_device_batch_size']}")
    print(f"â”œâ”€ gradient_accumulation_steps: {configs['gradient_accumulation_steps']}")
    print(f"â”œâ”€ lora_r: {configs['lora_r']}")
    print(f"â”œâ”€ lora_alpha: {configs['lora_alpha']}")
    print(f"â””â”€ dataloader_num_workers: {configs['dataloader_num_workers']}")
    
    # ì‹¤íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°
    effective_batch_size = configs['per_device_batch_size'] * configs['gradient_accumulation_steps']
    print(f"\nğŸ¯ ì‹¤íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {effective_batch_size}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
    estimated_memory = estimate_memory_usage(configs, total_memory)
    print(f"ğŸ“Š ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory:.1f}GB")
    
    if estimated_memory > total_memory * 0.95 / 1024:  # MB to GB
        print("âš ï¸  ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜! ì„¤ì •ì„ ë‚®ì¶°ì£¼ì„¸ìš”.")
    else:
        print("âœ… ë©”ëª¨ë¦¬ ì—¬ìœ  ì¶©ë¶„!")
    
    return configs

def estimate_memory_usage(configs, gpu_memory_mb):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
    base_model_memory = 3.5  # 8B ëª¨ë¸ 4bit ê¸°ì¤€ ì•½ 3.5GB
    
    # LoRA íŒŒë¼ë¯¸í„° ë©”ëª¨ë¦¬
    lora_memory = configs['lora_r'] * 0.01  # ëŒ€ëµì  ì¶”ì •
    
    # ë°°ì¹˜ ë° ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ ë©”ëª¨ë¦¬
    batch_memory = configs['per_device_batch_size'] * configs['max_seq_length'] * 0.001
    
    # Gradient checkpointing ê³ ë ¤
    gradient_memory = batch_memory * 0.5  # ëŒ€ëµ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
    
    total_estimated = base_model_memory + lora_memory + batch_memory + gradient_memory
    
    return total_estimated

def create_yaml_config(configs, output_file="auto_qlora_config.yaml"):
    """ìë™ ìƒì„±ëœ ì„¤ì •ì„ YAML íŒŒì¼ë¡œ ì €ì¥"""
    yaml_content = f"""### Auto-generated QLoRA Config for {configs['name']}
model_name: "allganize/Llama-3-Alpha-Ko-8B-Instruct"
dataset_path: "."
max_seq_length: {configs['max_seq_length']}
output_dir: "./llama-3.1-korean-8b-qlora-auto"
report_to: "wandb"

# ìµœì í™”ëœ í•™ìŠµ ì„¤ì •
learning_rate: 0.0002
lr_scheduler_type: "cosine"
num_train_epochs: 3
per_device_train_batch_size: {configs['per_device_batch_size']}
per_device_eval_batch_size: {configs['per_device_batch_size']}
gradient_accumulation_steps: {configs['gradient_accumulation_steps']}
optim: "paged_adamw_8bit"

# ë¡œê¹… ë° í‰ê°€
logging_steps: 5
save_strategy: "steps"
save_steps: 25
eval_strategy: "steps"
eval_steps: 25

# ì •ê·œí™”
weight_decay: 0.01
max_grad_norm: 1.0
warmup_ratio: 0.03

# ë©”ëª¨ë¦¬ ìµœì í™”
bf16: true
tf32: true
gradient_checkpointing: true
dataloader_num_workers: {configs['dataloader_num_workers']}
group_by_length: true
dataloader_pin_memory: false

# ëª¨ë¸ ì €ì¥
load_best_model_at_end: false
save_total_limit: 1

# QLoRA í™œì„±í™”
use_qlora: yes
lora_r: {configs['lora_r']}
lora_alpha: {configs['lora_alpha']}

# ê¸°íƒ€
remove_unused_columns: true
dataloader_drop_last: true
seed: 42
"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(yaml_content)
    
    print(f"ğŸ“„ ì„¤ì • íŒŒì¼ ì €ì¥: {output_file}")

if __name__ == "__main__":
    print("ğŸ” ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì¤‘...")
    
    # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒí™© ì²´í¬
    monitor_memory_usage()
    
    # ê¶Œì¥ ì„¤ì • ìƒì„±
    configs = get_recommended_settings()
    
    if configs:
        # YAML ì„¤ì • íŒŒì¼ ìë™ ìƒì„±
        create_yaml_config(configs)
        
        print(f"\nğŸš€ ì‚¬ìš©ë²•:")
        print(f"python train_qlora.py --config auto_qlora_config.yaml")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        memory_cleanup()
        print("\nâœ¨ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!")