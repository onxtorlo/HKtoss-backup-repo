{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d292c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d384cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def section_by_section(input_text: str, tokenizer, model) -> str:\n",
    "    print(\"ğŸš€ ì„¹ì…˜ë³„ ìƒì„± ì‹œì‘...\")\n",
    "    \n",
    "    sections = []\n",
    "    \n",
    "    # ê° ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸\n",
    "    section_prompts = [\n",
    "        {\n",
    "            \"name\": \"í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´\",\n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´:**\\n{{ì™„ì „í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ}}\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ê´€ê³„ ë°ì´í„°\", \n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë°ì´í„°ë² ì´ìŠ¤ ê´€ê³„ ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**ê´€ê³„ ë°ì´í„°:**\\n[ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ]\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ERD ë°ì´í„°\",\n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ERD ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**ERD ë°ì´í„°:**\\n[ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ]\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"API ëª…ì„¸\",\n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ API ëª…ì„¸ ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**API ëª…ì„¸ ë°ì´í„°:**\\n{{ì™„ì „í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ}}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, section in enumerate(section_prompts):\n",
    "        start_time = time.time()\n",
    "        print(f\"[{i+1}/4] {section['name']} ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        inputs = tokenizer(section['prompt'], return_tensors='pt', truncation=True, max_length=512)\n",
    "        eos_token_id = tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          outputs = model.generate(\n",
    "              input_ids=inputs[\"input_ids\"].to('cuda'),\n",
    "              max_new_tokens=2048,        # 3072 â†’ 2048 (ì ë‹¹íˆ ì¤„ì´ê¸°)\n",
    "              do_sample=False,            # True â†’ False (ê·¸ë¦¬ë””, ê°€ì¥ ë¹ ë¦„)\n",
    "              use_cache=True,             # ìºì‹œ ì‚¬ìš©\n",
    "              num_beams=1,               # ë¹” ì„œì¹˜ ë„ê¸°\n",
    "              early_stopping=False,\n",
    "              pad_token_id=tokenizer.eos_token_id,\n",
    "              eos_token_id=None\n",
    "          )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        section_content = response[len(section['prompt']):].strip()\n",
    "        sections.append(section_content)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    âœ… ì™„ë£Œ: {len(section_content)} ë¬¸ì, {elapsed:.1f}ì´ˆ\")\n",
    "    \n",
    "    # ëª¨ë“  ì„¹ì…˜ ê²°í•©\n",
    "    full_response = input_text + \"\\n\\nAssistant: \" + \"\\n\\n\".join(sections)\n",
    "    print(f\"ğŸ‰ ì „ì²´ ì™„ë£Œ! ì´ {len(full_response)} ë¬¸ì ìƒì„±\")\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "username=\"Min-kyu\"\n",
    "MODEL_NAME = \"PJA_LLM_MODEL_8bit\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('HUG_API_KEY')\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{username}/{MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"{username}/{MODEL_NAME}\")\n",
    "\n",
    "# ì¶”ë¡ \n",
    "input_text = \"\"\"\n",
    "ì´ í”„ë¡œì íŠ¸ëŠ” 3ì¸ì¹­ ì¹´ë“œê²Œì„ê¸°ë°˜ MMORPG ê²Œì„ ê°œë°œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì°½ì—…ì„ ëª©ì ìœ¼ë¡œ ê²Œì„ ë°ì´í„°ë¥¼ ëŒì–´ë‹¤ê°€ í”„ë¡œì íŠ¸ë¥¼ í•˜ë ¤ê³ í•©ë‹ˆë‹¤. ì£¼ì œëŠ” ê°„ë‹¨í•˜ê²Œ 3ì¸ì¹­ ì¹´ë“œê²Œì„ì„ mmorpgí˜•ì‹ìœ¼ë¡œ ë§Œë“œë ¤ê³  í•©ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë§Œë“¤ìƒê°ì´ëƒë©´ ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ê²Œ í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ê²Œì„ì„ ë§Œë“¤ê³ ì‹¶ìŠµë‹ˆë‹¤. ë˜í•œ, ìŠ¤í† ë¦¬ê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ê³  ì„ íƒí•˜ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ì— ë”°ë¼ì„œ ë“œë¡­ë˜ëŠ” ì¹´ë“œì˜ í˜•ì‹ì´ ë‹¬ëìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì´ ê²Œì„ì„ í†µí•´ì„œ ì‚¬ìš©ìê°€ ëœë¤ ê°€ì±  + ìˆœê°„ì ì¸ íŒë‹¨ìœ¼ë¡œ ì»¨íŠ¸ë¡¤í•˜ëŠ” ëŠ¥ë ¥ì´ ëŠ˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ê¸°íší•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].to('cuda'),\n",
    "        max_new_tokens=500,\n",
    "        eos_token_id=eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response.replace(\"', '\", \"',\\n'\").replace(\"}, {\", \"},\\n{\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476a3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('HUG_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf06c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b18a308c2f45e28cadc4cd0fb6170c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì´ í”„ë¡œì íŠ¸ëŠ” 3ì¸ì¹­ ì¹´ë“œê²Œì„ê¸°ë°˜ MMORPG ê²Œì„ ê°œë°œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì°½ì—…ì„ ëª©ì ìœ¼ë¡œ ê²Œì„ ë°ì´í„°ë¥¼ ëŒì–´ë‹¤ê°€ í”„ë¡œì íŠ¸ë¥¼ í•˜ë ¤ê³ í•©ë‹ˆë‹¤. ì£¼ì œëŠ” ê°„ë‹¨í•˜ê²Œ 3ì¸ì¹­ ì¹´ë“œê²Œì„ì„ mmorpgí˜•ì‹ìœ¼ë¡œ ë§Œë“œë ¤ê³  í•©ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë§Œë“¤ìƒê°ì´ëƒë©´ ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ê²Œ í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ê²Œì„ì„ ë§Œë“¤ê³ ì‹¶ìŠµë‹ˆë‹¤. ë˜í•œ, ìŠ¤í† ë¦¬ê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ê³  ì„ íƒí•˜ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ì— ë”°ë¼ì„œ ë“œë¡­ë˜ëŠ” ì¹´ë“œì˜ í˜•ì‹ì´ ë‹¬ëìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì´ ê²Œì„ì„ í†µí•´ì„œ ì‚¬ìš©ìê°€ ëœë¤ ê°€ì±  + ìˆœê°„ì ì¸ íŒë‹¨ìœ¼ë¡œ ì»¨íŠ¸ë¡¤í•˜ëŠ” ëŠ¥ë ¥ì´ ëŠ˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ê¸°íší•˜ì˜€ìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "Assistant: **í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´:**\n",
      "{'project_summary': {'title': '3ì¸ì¹­ ì¹´ë“œê²Œì„ ê¸°ë°˜ MMORPG', 'category': 'ê²Œì„', 'target_users': ['ê²Œì„ ì• í˜¸ê°€', 'MMORPG íŒ¬', 'ì¹´ë“œê²Œì„ ìœ ì €'],'main_purpose': 'ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ê²Œí•˜ëŠ” ìƒˆë¡œìš´ í˜•ì‹ì˜ MMORPG ê°œë°œ', 'key_features': [{'feature': 'ì¹´ë“œ ë“œë¡­', 'description': 'ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ëŠ” ê¸°ëŠ¥'}, {'feature': 'ìŠ¤í† ë¦¬', 'description': 'ì‚¬ìš©ìì˜ ì„ íƒì— ë”°ë¼ ë“œë¡­ë˜ëŠ” ì¹´ë“œì˜ í˜•ì‹ì´ ë‹¬ë¼ì§€ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ ì œê³µ'}, {'feature': 'ì„ íƒ ìŠ¤í† ë¦¬ë¼ì¸', 'description': 'ì‚¬ìš©ìê°€ ì„ íƒí•˜ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ì— ë”°ë¼ ì¹´ë“œì˜ í˜•ì‹ì´ ë‹¬ë¼ì§'}, {'feature': 'ì»¨íŠ¸ë¡¤ ëŠ¥ë ¥ ê°•í™”', 'description': 'ëœë¤ ê°€ì± ê³¼ ìˆœê°„ì ì¸ íŒë‹¨ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì»¨íŠ¸ë¡¤í•˜ëŠ” ëŠ¥ë ¥ ì¦ëŒ€'}], 'core_technologies': [{'category': 'Game Development', 'technologies': ['Unity', 'Unet', '2D/3D Animation']}, {'category': 'Game UI', 'technologies': ['UI/UX Design', 'Sprite Rendering']}, {'category': 'Audio', 'technologies': ['BGM', 'SFX']}, {'category': 'AI', 'technologies': ['ì •í™•ë„ í–¥ìƒ', 'ì‹¤íš¨ì‚¬', 'ì‹¤ì‹œê°„ ë°˜ì‘']}], 'problem_solving': {'current_problem': 'ê¸°ì¡´ RPG ê²Œì„ì˜ ë°˜ë³µì ì´ê³  ì§€ë£¨í•œ ì½˜í…ì¸ ','solution_approach': '3ì¸ì¹­ ì¹´ë“œê²Œì„ì˜ ë‚œë¦¬í•¨ê³¼ ìŠ¤í† ë¦¬ì  ì¸ì‚¬ì´íŠ¸ë¥¼ í†µí•´ í¥ë¯¸ ìˆ˜í˜¸', 'expected_benefits': ['ì‚¬ìš©ì ì°¸ì—¬ ì¦ëŒ€', 'í”Œë ˆì´ì–´ì˜ í¥ë¯¸ ìœ ì§€', 'ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ ì½˜í…ì¸  ì¬ë¶€í™”']},'special_features': ['ëœë¤ ê°€ì± ì˜ í¥ë¯¸ ìš”ì†Œ ì¬ë°°ì¹˜', 'ì‹¤ì‹œê°„ íŒë‹¨ë ¥ ìš”êµ¬', 'ì‚¬ìš©ì ì°¸ì—¬ì˜ ëŒë¦¬ê¸°']}, 'business_model': {'type': 'Freemium\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{username}/{MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"{username}/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccbc3275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì´ í”„ë¡œì íŠ¸ëŠ” 3ì¸ì¹­ ì¹´ë“œê²Œì„ê¸°ë°˜ MMORPG ê²Œì„ ê°œë°œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì°½ì—…ì„ ëª©ì ìœ¼ë¡œ ê²Œì„ ë°ì´í„°ë¥¼ ëŒì–´ë‹¤ê°€ í”„ë¡œì íŠ¸ë¥¼ í•˜ë ¤ê³ í•©ë‹ˆë‹¤. ì£¼ì œëŠ” ê°„ë‹¨í•˜ê²Œ 3ì¸ì¹­ ì¹´ë“œê²Œì„ì„ mmorpgí˜•ì‹ìœ¼ë¡œ ë§Œë“œë ¤ê³  í•©ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë§Œë“¤ìƒê°ì´ëƒë©´ ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ê²Œ í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ê²Œì„ì„ ë§Œë“¤ê³ ì‹¶ìŠµë‹ˆë‹¤. ë˜í•œ, ìŠ¤í† ë¦¬ê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ê³  ì„ íƒí•˜ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ì— ë”°ë¼ì„œ ë“œë¡­ë˜ëŠ” ì¹´ë“œì˜ í˜•ì‹ì´ ë‹¬ëìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì´ ê²Œì„ì„ í†µí•´ì„œ ì‚¬ìš©ìê°€ ëœë¤ ê°€ì±  + ìˆœê°„ì ì¸ íŒë‹¨ìœ¼ë¡œ ì»¨íŠ¸ë¡¤í•˜ëŠ” ëŠ¥ë ¥ì´ ëŠ˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ê¸°íší•˜ì˜€ìŠµë‹ˆë‹¤.\n",
      "ì‚¬ìš© ê¸°ìˆ ë¡œëŠ” Python, Node.js, Express, Django, Flask, Twilio, Kahoot, etcì„ ì‚¬ìš©í•  ê³„íšì…ë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ëŠ” MongoDB, FireSQLë¡œ ì„¤ê³„í•  ê²ƒì´ê³  APIë¡œëŠ” RESTful API, GraphQL APIë¥¼ ì‚¬ìš©í•  ê³„íšì…ë‹ˆë‹¤. ê¸°ìˆ ì ì´ê³  ì‹¤ìš©ì ì¸ ê´€ì ì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ë¶„ì„í•˜ë©°, ê°œë°œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë¹ ë¥´ê²Œ ì‹¤í–‰ì— ì˜®ê¸¸ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ê°€ì¹˜ë¥¼ ì œê³µí• ê²ë‹ˆë‹¤.\n",
      "\n",
      "Assistant: **í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´:**\n",
      "{'project_summary': {'title': '3ì¸ì¹­ ì¹´ë“œê²Œì„ ê¸°ë°˜ MMORPG ê²Œì„',\n",
      "'category': 'MMORPG ê²Œì„',\n",
      "'target_users': ['ê²Œì„ ì• í˜¸ê°€',\n",
      "'ì¹´ë“œê²Œì„ íŒ¬',\n",
      "'ëŒ€ì¤‘ ê²Œì„ ì‚¬ìš©ì'],'main_purpose': '3ì¸ì¹­ ì¹´ë“œê²Œì„ì„ mmorpg í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©ìì˜ ê²Œì„ ê²½í—˜ì„ ì¦ê°€ì‹œí‚¤ê¸°',\n",
      "'key_features': [{'feature': 'ì¹´ë“œ ë“œë¡­ ì‹œìŠ¤í…œ',\n",
      "'description': 'ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ëŠ” í˜•ì‹'},\n",
      "{'feature': 'ìŠ¤í† ë¦¬',\n",
      "'description': 'ì„ íƒí•˜ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ì— ë”°ë¼ ë“œë¡­ë˜ëŠ” ì¹´ë“œì˜ í˜•ì‹ ë‹¬ë¼ì§'},\n",
      "{'feature': 'ì‚¬ìš©ì ì»¨íŠ¸ë¡¤',\n",
      "'description': 'ëœë“œê°€ ì±  ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒë‹¨ ìˆ˜í–‰'}], 'core_technologies': [{'category': 'Programming',\n",
      "'technologies': ['Python',\n",
      "'Node.js']},\n",
      "{'category': 'Web Framework',\n",
      "'technologies': ['Express',\n",
      "'Django',\n",
      "'Flask']},\n",
      "{'category': 'Database',\n",
      "'technologies': ['MongoDB',\n",
      "'FireSQL']},\n",
      "{'category': 'API Framework',\n",
      "'technologies': ['RESTful API',\n",
      "'GraphQL API']},\n",
      "{'category': 'Notification',\n",
      "'technologies': ['Twilio']},\n",
      "{'category': 'Quiz',\n",
      "'technologies': ['Kahoot']}], 'problem_solving': {'current_problem': 'ë¹„íš¨ìœ¨ì ì¸ ê²Œì„ ë°ì´í„° í™œìš© ë° ì‚¬ìš©ì ì°¸ì—¬ ìœ ë„ ë¶€ì¡±','solution_approach': '3ì¸ì¹­ ì¹´ë“œê²Œì„ì„ í†µí•´ ì‚¬ìš©ì ì°¸ì—¬ ì´‰ì§„ ë° ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬',\n",
      "'expected_benefits': ['ì‚¬ìš©ì ì°¸ì—¬ ì¦ê°€',\n",
      "'ê²Œì„ ê²½í—˜ í–¥ìƒ',\n",
      "'ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íš¨ìœ¨ì„±']},'special\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ë¡ \n",
    "input_text = \"\"\"\n",
    "ì´ í”„ë¡œì íŠ¸ëŠ” 3ì¸ì¹­ ì¹´ë“œê²Œì„ê¸°ë°˜ MMORPG ê²Œì„ ê°œë°œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì°½ì—…ì„ ëª©ì ìœ¼ë¡œ ê²Œì„ ë°ì´í„°ë¥¼ ëŒì–´ë‹¤ê°€ í”„ë¡œì íŠ¸ë¥¼ í•˜ë ¤ê³ í•©ë‹ˆë‹¤. ì£¼ì œëŠ” ê°„ë‹¨í•˜ê²Œ 3ì¸ì¹­ ì¹´ë“œê²Œì„ì„ mmorpgí˜•ì‹ìœ¼ë¡œ ë§Œë“œë ¤ê³  í•©ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë§Œë“¤ìƒê°ì´ëƒë©´ ì‹œê°„ì´ 10ì´ˆ ì§€ë‚  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ì¹´ë“œê°€ ë“œë¡­ë˜ê²Œ í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ê²Œì„ì„ ë§Œë“¤ê³ ì‹¶ìŠµë‹ˆë‹¤. ë˜í•œ, ìŠ¤í† ë¦¬ê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ê³  ì„ íƒí•˜ëŠ” ìŠ¤í† ë¦¬ë¼ì¸ì— ë”°ë¼ì„œ ë“œë¡­ë˜ëŠ” ì¹´ë“œì˜ í˜•ì‹ì´ ë‹¬ëìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì´ ê²Œì„ì„ í†µí•´ì„œ ì‚¬ìš©ìê°€ ëœë¤ ê°€ì±  + ìˆœê°„ì ì¸ íŒë‹¨ìœ¼ë¡œ ì»¨íŠ¸ë¡¤í•˜ëŠ” ëŠ¥ë ¥ì´ ëŠ˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ê¸°íší•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].to('cuda'),\n",
    "        max_new_tokens=500,\n",
    "        eos_token_id=eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response.replace(\"', '\", \"',\\n'\").replace(\"}, {\", \"},\\n{\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae10007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def section_by_section(input_text: str, tokenizer, model) -> str:\n",
    "    print(\"ğŸš€ ì„¹ì…˜ë³„ ìƒì„± ì‹œì‘...\")\n",
    "    \n",
    "    sections = []\n",
    "    \n",
    "    # ê° ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸\n",
    "    section_prompts = [\n",
    "        {\n",
    "            \"name\": \"í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´\",\n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´:**\\n{{ì™„ì „í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ}}\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ê´€ê³„ ë°ì´í„°\", \n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë°ì´í„°ë² ì´ìŠ¤ ê´€ê³„ ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**ê´€ê³„ ë°ì´í„°:**\\n[ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ]\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ERD ë°ì´í„°\",\n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ERD ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**ERD ë°ì´í„°:**\\n[ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ]\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"API ëª…ì„¸\",\n",
    "            \"prompt\": f\"{input_text}\\n\\nìœ„ í”„ë¡œì íŠ¸ì— ëŒ€í•´ API ëª…ì„¸ ë°ì´í„°ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”:\\n**API ëª…ì„¸ ë°ì´í„°:**\\n{{ì™„ì „í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ}}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, section in enumerate(section_prompts):\n",
    "        start_time = time.time()\n",
    "        print(f\"[{i+1}/4] {section['name']} ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        inputs = tokenizer(section['prompt'], return_tensors='pt', truncation=True, max_length=512)\n",
    "        eos_token_id = tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          outputs = model.generate(\n",
    "              input_ids=inputs[\"input_ids\"].to('cuda'),\n",
    "              max_new_tokens=2048,        # 3072 â†’ 2048 (ì ë‹¹íˆ ì¤„ì´ê¸°)\n",
    "              do_sample=False,            # True â†’ False (ê·¸ë¦¬ë””, ê°€ì¥ ë¹ ë¦„)\n",
    "              use_cache=True,             # ìºì‹œ ì‚¬ìš©\n",
    "              num_beams=1,               # ë¹” ì„œì¹˜ ë„ê¸°\n",
    "              early_stopping=False,\n",
    "              pad_token_id=tokenizer.eos_token_id,\n",
    "              eos_token_id=None\n",
    "          )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        section_content = response[len(section['prompt']):].strip()\n",
    "        sections.append(section_content)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    âœ… ì™„ë£Œ: {len(section_content)} ë¬¸ì, {elapsed:.1f}ì´ˆ\")\n",
    "    \n",
    "    # ëª¨ë“  ì„¹ì…˜ ê²°í•©\n",
    "    full_response = input_text + \"\\n\\nAssistant: \" + \"\\n\\n\".join(sections)\n",
    "    print(f\"ğŸ‰ ì „ì²´ ì™„ë£Œ! ì´ {len(full_response)} ë¬¸ì ìƒì„±\")\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec867cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì„¹ì…˜ë³„ ìƒì„± ì‹œì‘...\n",
      "[1/4] í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´ ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msection_by_section\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36msection_by_section\u001b[0;34m(input_text, tokenizer, model)\u001b[0m\n\u001b[1;32m     33\u001b[0m eos_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 3072 â†’ 2048 (ì ë‹¹íˆ ì¤„ì´ê¸°)\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# True â†’ False (ê·¸ë¦¬ë””, ê°€ì¥ ë¹ ë¦„)\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# ìºì‹œ ì‚¬ìš©\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# ë¹” ì„œì¹˜ ë„ê¸°\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m section_content \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;28mlen\u001b[39m(section[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]):]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:734\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    670\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 672\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:797\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 797\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCxB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;66;03m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;66;03m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:556\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    555\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:398\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    395\u001b[0m out32, Sout32 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39migemmlt(C32A, state\u001b[38;5;241m.\u001b[39mCxB, SA, state\u001b[38;5;241m.\u001b[39mSB)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m bias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# we apply the fused bias here\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm_dequant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSout32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# apply bias separately\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py:2353\u001b[0m, in \u001b[0;36mmm_dequant\u001b[0;34m(A, quant_state, row_stats, col_stats, out, new_row_stats, new_col_stats, bias)\u001b[0m\n\u001b[1;32m   2350\u001b[0m     out_shape \u001b[38;5;241m=\u001b[39m (out_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m out_shape[\u001b[38;5;241m1\u001b[39m], out_shape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2353\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_row_stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2355\u001b[0m     new_row_stats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(out_shape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = section_by_section(input_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac944f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
